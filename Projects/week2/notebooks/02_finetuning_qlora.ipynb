{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 — Notebook 2: QLoRA Fine-Tuning (SFT Warm-up → DPO)\n",
    "\n",
    "**Strategy:** Two-phase training to counteract the alignment tax observed in production:\n",
    "\n",
    "1. **Phase 1 — SFT warm-up** on the 700 highest-quality correct samples  \n",
    "   → anchors the model in the correct factual space before preference learning\n",
    "2. **Phase 2 — DPO** on chosen/rejected pairs from Notebook 01  \n",
    "   → with raised KL penalty (β) to prevent over-optimization\n",
    "\n",
    "---\n",
    "> **GPU requirement:** 1× A100/H100 40GB+ recommended. Works on 2× A40 with `load_in_4bit=True`.  \n",
    "> **Default model:** `Qwen/Qwen2.5-7B-Instruct` (swap in Llama-3 or Mistral as needed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Install & Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yz4q8ic2qun",
   "source": "## 0. Colab Setup (skip if running locally)\n\nRun this cell first. It clones the repo, sets paths, and installs dependencies.  \nIn Colab Pro: **Runtime → Change runtime type → GPU → A100** before running.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "k4yq76fyl2h",
   "source": "import sys, os\n\nIN_COLAB = \"google.colab\" in sys.modules\nif IN_COLAB:\n    # GPU check\n    os.system(\"nvidia-smi --query-gpu=name,memory.total --format=csv,noheader\")\n\n    # Install training stack\n    os.system(\"pip install -q transformers>=4.40 peft trl>=0.8 bitsandbytes accelerate datasets wandb python-dotenv\")\n\n    # Clone repo\n    if not os.path.exists(\"/content/agentic-ai-learning\"):\n        os.system(\"git clone -q https://github.com/amnghd/agentic-ai-learning.git /content/agentic-ai-learning\")\n\n    os.chdir(\"/content/agentic-ai-learning/Projects/week2/notebooks\")\n    os.makedirs(\"../data\", exist_ok=True)\n    os.makedirs(\"../models\", exist_ok=True)\n\n    # NB02 needs dpo_floor.jsonl produced by NB01.\n    # If not present, run NB01 first OR download a pre-built copy from Drive:\n    if not os.path.exists(\"../data/dpo_floor.jsonl\"):\n        print(\"⚠  ../data/dpo_floor.jsonl not found.\")\n        print(\"   Run NB01 first in this session, or mount Drive and copy the file:\")\n        print(\"   from google.colab import drive; drive.mount('/content/drive')\")\n        print(\"   !cp '/content/drive/MyDrive/agentic/dpo_floor.jsonl' ../data/\")\n    else:\n        print(\"✓  dpo_floor.jsonl found.\")\n\n    print(\"Colab setup complete. Working dir:\", os.getcwd())\nelse:\n    print(\"Running locally — no setup needed.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers>=4.40 peft trl>=0.8 bitsandbytes accelerate datasets wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../../week1/.env\")\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, DPOTrainer, DPOConfig\n",
    "from datasets import Dataset, load_from_disk\n",
    "\n",
    "DATA_DIR   = Path(\"../data\")\n",
    "MODELS_DIR = Path(\"../models\")\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kiv8jzbnjq",
   "source": "## Live Metrics Tracker\n\nLogs **loss**, **perplexity** (`exp(loss)`), and **normalized perplexity** every `LOG_EVERY` steps for both SFT and DPO phases. Plot inline after each phase.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "t9sf4b5k38",
   "source": "import math\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nfrom transformers import TrainerCallback\n\nLOG_EVERY = 5   # record metrics every N steps — lower = smoother curve, slightly slower\n\n\nclass MetricsCallback(TrainerCallback):\n    \"\"\"Records loss, perplexity, and normalised perplexity every LOG_EVERY steps.\"\"\"\n\n    def __init__(self, label: str = \"train\"):\n        self.label = label\n        self.steps, self.losses, self.perplexities = [], [], []\n        self._norm_base = None   # set after training for normalisation\n\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if logs is None or state.global_step % LOG_EVERY != 0:\n            return\n        loss = logs.get(\"loss\") or logs.get(\"train_loss\")\n        if loss is None:\n            return\n        ppl = math.exp(min(loss, 20))   # cap to avoid overflow on early steps\n        self.steps.append(state.global_step)\n        self.losses.append(loss)\n        self.perplexities.append(ppl)\n\n    def normalised_perplexity(self):\n        \"\"\"Scale perplexity to [0, 1] using the first value as the reference max.\"\"\"\n        if not self.perplexities:\n            return []\n        base = self.perplexities[0] or 1.0\n        return [p / base for p in self.perplexities]\n\n    def plot(self, title: str = \"\"):\n        if not self.steps:\n            print(\"No data to plot yet.\")\n            return\n        norm_ppl = self.normalised_perplexity()\n        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n        fig.suptitle(title or self.label, fontsize=13)\n\n        for ax, values, ylabel, color in zip(\n            axes,\n            [self.losses, self.perplexities, norm_ppl],\n            [\"Loss\", \"Perplexity\", \"Normalised Perplexity (PPL / PPL₀)\"],\n            [\"steelblue\", \"darkorange\", \"seagreen\"],\n        ):\n            ax.plot(self.steps, values, color=color, linewidth=1.8)\n            ax.set_xlabel(\"Step\")\n            ax.set_ylabel(ylabel)\n            ax.xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n            ax.grid(True, alpha=0.3)\n\n        plt.tight_layout()\n        plt.show()\n        print(f\"  Final loss: {self.losses[-1]:.4f} | \"\n              f\"PPL: {self.perplexities[-1]:.2f} | \"\n              f\"Norm PPL: {norm_ppl[-1]:.3f}\")\n\n\n# Instantiate one tracker per phase — reuse across cells\nsft_metrics = MetricsCallback(\"SFT warm-up\")\ndpo_metrics = MetricsCallback(\"DPO\")\nprint(f\"MetricsCallback ready — logging every {LOG_EVERY} steps.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Config\n\nPick any model below — all run on a single consumer GPU (16GB VRAM) without 4-bit quantization,\nor on a free Colab T4 with `load_in_4bit=True`.\n\n```\n# ≤ 2B options (recommended for fast iteration)\n\"google/gemma-2-2b-it\"                  # strongest benchmarks at 2B\n\"Qwen/Qwen2.5-1.5B-Instruct\"           # best instruction-following at 1.5B\n\"meta-llama/Llama-3.2-1B-Instruct\"     # Meta official 1B\n\"HuggingFaceTB/SmolLM2-1.7B-Instruct\"  # HF's efficient 1.7B\n\n# 7B options (need A40/A100 or 4-bit on 16GB)\n\"Qwen/Qwen2.5-7B-Instruct\"\n\"meta-llama/Meta-Llama-3-8B-Instruct\"\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Model ────────────────────────────────────────────────────────────────────\nBASE_MODEL = \"google/gemma-2-2b-it\"      # default: strong 2B, fits 16GB without quantization\n# BASE_MODEL = \"Qwen/Qwen2.5-1.5B-Instruct\"\n# BASE_MODEL = \"meta-llama/Llama-3.2-1B-Instruct\"\n# BASE_MODEL = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n# BASE_MODEL = \"Qwen/Qwen2.5-7B-Instruct\"          # needs A40/A100 or 4-bit\n\nRUN_NAME = BASE_MODEL.split(\"/\")[-1] + \"-dpo\"\n\n# ── QLoRA ────────────────────────────────────────────────────────────────────\nLORA_R       = 16\nLORA_ALPHA   = 32\nLORA_DROPOUT = 0.05\n# Gemma-2 / Llama-3 / Qwen all share these projection names\nLORA_TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                        \"gate_proj\", \"up_proj\", \"down_proj\"]\n\n# ── SFT warm-up ──────────────────────────────────────────────────────────────\nSFT_EPOCHS      = 1\nSFT_BATCH_SIZE  = 4\nSFT_LR          = 2e-4\nSFT_MAX_SEQ_LEN = 1024\n\n# ── DPO ──────────────────────────────────────────────────────────────────────\nDPO_BETA       = 0.2   # raised from default 0.1 to reduce alignment tax\nDPO_EPOCHS     = 1\nDPO_BATCH_SIZE = 2\nDPO_LR         = 5e-5\nDPO_MAX_LENGTH = 1024\n\nprint(f\"Base model : {BASE_MODEL}\")\nprint(f\"DPO beta   : {DPO_BETA}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model in 4-bit (QLoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "print(f\"Model loaded: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    target_modules=LORA_TARGET_MODULES,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "sft_args = TrainingArguments(\n    output_dir=str(MODELS_DIR / f\"{RUN_NAME}-sft\"),\n    num_train_epochs=SFT_EPOCHS,\n    per_device_train_batch_size=SFT_BATCH_SIZE,\n    gradient_accumulation_steps=4,\n    learning_rate=SFT_LR,\n    fp16=False,\n    bf16=True,\n    logging_steps=LOG_EVERY,\n    save_strategy=\"epoch\",\n    optim=\"paged_adamw_8bit\",\n    report_to=\"wandb\" if os.getenv(\"WANDB_API_KEY\") else \"none\",\n    run_name=f\"{RUN_NAME}-sft\",\n)\n\nsft_trainer = SFTTrainer(\n    model=model,\n    args=sft_args,\n    train_dataset=ds_sft,\n    tokenizer=tokenizer,\n    dataset_text_field=\"text\",\n    max_seq_length=SFT_MAX_SEQ_LEN,\n    callbacks=[sft_metrics],\n)\n\nprint(\"Starting SFT warm-up...\")\nsft_trainer.train()\nsft_trainer.save_model(str(MODELS_DIR / f\"{RUN_NAME}-sft-final\"))\nprint(\"SFT complete.\")"
  },
  {
   "cell_type": "code",
   "id": "tzpem0ymq",
   "source": "sft_metrics.plot(f\"SFT Warm-up — {BASE_MODEL}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\n\n# Load pairs produced by NB01 (floor strategy: score_chosen >= 0.70, delta >= 0.10)\ndf_dpo = pd.read_json(DATA_DIR / \"dpo_floor.jsonl\", lines=True)\n\n# SFT warm-up: use only the highest-scoring chosen responses\n# score_chosen was saved alongside prompt/chosen/rejected in NB01\nSFT_SCORE_FLOOR = 0.80\nif \"score_chosen\" in df_dpo.columns:\n    df_sft = df_dpo[df_dpo[\"score_chosen\"] >= SFT_SCORE_FLOOR].copy()\nelse:\n    # Fallback: top quartile by position if score not present in file\n    df_sft = df_dpo.nlargest(min(700, len(df_dpo)), \"score_chosen\") if \"score_chosen\" in df_dpo.columns \\\n             else df_dpo.sample(min(700, len(df_dpo)), random_state=42)\n\nprint(f\"SFT warm-up candidates: {len(df_sft):,}\")\n\n\ndef to_sft_text(row):\n    \"\"\"Format as chat-style text for causal LM training.\"\"\"\n    return f\"### User:\\n{row['prompt']}\\n\\n### Assistant:\\n{row['chosen']}\"\n\n\ndf_sft[\"text\"] = df_sft.apply(to_sft_text, axis=1)\nds_sft = Dataset.from_pandas(df_sft[[\"text\"]])\n\nprint(f\"SFT warm-up samples: {len(ds_sft):,}\")\nprint(ds_sft[0][\"text\"][:300])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "dpo_config = DPOConfig(\n    beta=DPO_BETA,\n    output_dir=str(MODELS_DIR / f\"{RUN_NAME}-dpo\"),\n    num_train_epochs=DPO_EPOCHS,\n    per_device_train_batch_size=DPO_BATCH_SIZE,\n    per_device_eval_batch_size=DPO_BATCH_SIZE,\n    gradient_accumulation_steps=8,\n    learning_rate=DPO_LR,\n    bf16=True,\n    logging_steps=LOG_EVERY,\n    evaluation_strategy=\"steps\",\n    eval_steps=50,\n    save_strategy=\"steps\",\n    save_steps=100,\n    max_length=DPO_MAX_LENGTH,\n    max_prompt_length=512,\n    optim=\"paged_adamw_8bit\",\n    report_to=\"wandb\" if os.getenv(\"WANDB_API_KEY\") else \"none\",\n    run_name=f\"{RUN_NAME}-dpo\",\n)\n\ndpo_trainer = DPOTrainer(\n    model=model,\n    ref_model=None,\n    args=dpo_config,\n    train_dataset=split[\"train\"],\n    eval_dataset=split[\"test\"],\n    tokenizer=tokenizer,\n    callbacks=[dpo_metrics],\n)\n\nprint(f\"Starting DPO (β={DPO_BETA})...\")\ndpo_trainer.train()\ndpo_trainer.save_model(str(MODELS_DIR / f\"{RUN_NAME}-dpo-final\"))\nprint(\"DPO complete.\")"
  },
  {
   "cell_type": "code",
   "id": "0p45y6dnqjt",
   "source": "dpo_metrics.plot(f\"DPO (β={DPO_BETA}) — {BASE_MODEL}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "98acxhqb7t",
   "source": "## SFT vs DPO — Combined Learning Curves",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "86bdpixffr4",
   "source": "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\nfig.suptitle(f\"SFT vs DPO Learning Curves — {BASE_MODEL}\", fontsize=13)\n\nmetrics_pairs = [\n    (\"Loss\",                             \"losses\"),\n    (\"Perplexity\",                       \"perplexities\"),\n    (\"Normalised Perplexity (PPL / PPL₀)\", None),\n]\n\nfor ax, (ylabel, attr) in zip(axes, metrics_pairs):\n    for tracker, color, label in [\n        (sft_metrics, \"steelblue\", \"SFT\"),\n        (dpo_metrics, \"darkorange\", f\"DPO β={DPO_BETA}\"),\n    ]:\n        values = getattr(tracker, attr) if attr else tracker.normalised_perplexity()\n        if values:\n            ax.plot(tracker.steps, values, color=color, label=label, linewidth=1.8)\n    ax.set_xlabel(\"Step\")\n    ax.set_ylabel(ylabel)\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Phase 2 — DPO with Raised β\n",
    "\n",
    "`β = 0.2` (vs. default 0.1) → stronger KL penalty → model stays closer to SFT distribution, reducing correctness decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DPO pairs\n",
    "ds_dpo = Dataset.from_pandas(df_dpo[[\"prompt\", \"chosen\", \"rejected\"]])\n",
    "split   = ds_dpo.train_test_split(test_size=0.05, seed=42)\n",
    "\n",
    "print(f\"DPO train: {len(split['train']):,}  |  eval: {len(split['test']):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_config = DPOConfig(\n",
    "    beta=DPO_BETA,\n",
    "    output_dir=str(MODELS_DIR / f\"{RUN_NAME}-dpo\"),\n",
    "    num_train_epochs=DPO_EPOCHS,\n",
    "    per_device_train_batch_size=DPO_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=DPO_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=DPO_LR,\n",
    "    bf16=True,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    max_length=DPO_MAX_LENGTH,\n",
    "    max_prompt_length=512,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=\"wandb\" if os.getenv(\"WANDB_API_KEY\") else \"none\",\n",
    "    run_name=f\"{RUN_NAME}-dpo\",\n",
    ")\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=None,   # None = use frozen SFT checkpoint as reference\n",
    "    args=dpo_config,\n",
    "    train_dataset=split[\"train\"],\n",
    "    eval_dataset=split[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(f\"Starting DPO (β={DPO_BETA})...\")\n",
    "dpo_trainer.train()\n",
    "dpo_trainer.save_model(str(MODELS_DIR / f\"{RUN_NAME}-dpo-final\"))\n",
    "print(\"DPO complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quick Inference Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "test_prompt = \"My order hasn't arrived after 2 weeks. What should I do?\"\n",
    "inputs = tokenizer(\n",
    "    f\"### User:\\n{test_prompt}\\n\\n### Assistant:\\n\",\n",
    "    return_tensors=\"pt\"\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(output[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "print(\"Prompt:\", test_prompt)\n",
    "print(\"\\nResponse:\\n\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Phase | Method | Key setting | Purpose |\n",
    "|-------|--------|------------|--------|\n",
    "| 1 | SFT warm-up | 700 high-correctness samples | Anchor factual quality |\n",
    "| 2 | DPO | β=0.2 (raised KL) | Style alignment without correctness decay |\n",
    "\n",
    "**Next:** `03_model_evaluation.ipynb` — measure correctness, groundedness, style before and after."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
