{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 — Notebook 2: QLoRA Fine-Tuning (SFT Warm-up → DPO)\n",
    "\n",
    "**Strategy:** Two-phase training to counteract the alignment tax observed in production:\n",
    "\n",
    "1. **Phase 1 — SFT warm-up** on the 700 highest-quality correct samples  \n",
    "   → anchors the model in the correct factual space before preference learning\n",
    "2. **Phase 2 — DPO** on chosen/rejected pairs from Notebook 01  \n",
    "   → with raised KL penalty (β) to prevent over-optimization\n",
    "\n",
    "---\n",
    "> **GPU requirement:** 1× A100/H100 40GB+ recommended. Works on 2× A40 with `load_in_4bit=True`.  \n",
    "> **Default model:** `Qwen/Qwen2.5-7B-Instruct` (swap in Llama-3 or Mistral as needed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Install & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers>=4.40 peft trl>=0.8 bitsandbytes accelerate datasets wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../../week1/.env\")\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, DPOTrainer, DPOConfig\n",
    "from datasets import Dataset, load_from_disk\n",
    "\n",
    "DATA_DIR   = Path(\"../data\")\n",
    "MODELS_DIR = Path(\"../models\")\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Model ────────────────────────────────────────────────────────────────────\n",
    "BASE_MODEL = \"Qwen/Qwen2.5-7B-Instruct\"   # swap to: meta-llama/Meta-Llama-3-8B-Instruct\n",
    "RUN_NAME   = \"qwen2.5-7b-customer-support-dpo\"\n",
    "\n",
    "# ── QLoRA ────────────────────────────────────────────────────────────────────\n",
    "LORA_R          = 16\n",
    "LORA_ALPHA      = 32\n",
    "LORA_DROPOUT    = 0.05\n",
    "LORA_TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                        \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "# ── SFT warm-up ──────────────────────────────────────────────────────────────\n",
    "SFT_EPOCHS      = 1\n",
    "SFT_BATCH_SIZE  = 4\n",
    "SFT_LR          = 2e-4\n",
    "SFT_MAX_SEQ_LEN = 1024\n",
    "\n",
    "# ── DPO ──────────────────────────────────────────────────────────────────────\n",
    "DPO_BETA        = 0.2   # ↑ from default 0.1 to suppress over-optimization / alignment tax\n",
    "DPO_EPOCHS      = 1\n",
    "DPO_BATCH_SIZE  = 2\n",
    "DPO_LR          = 5e-5\n",
    "DPO_MAX_LENGTH  = 1024\n",
    "\n",
    "print(f\"Base model : {BASE_MODEL}\")\n",
    "print(f\"DPO beta   : {DPO_BETA}  (raised to mitigate alignment tax)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model in 4-bit (QLoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "print(f\"Model loaded: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    target_modules=LORA_TARGET_MODULES,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Phase 1 — SFT Warm-up on High-Score Chosen Responses\n\nFilter to rows where `score_chosen >= 0.80` (top quality bar from NB01) for the SFT pass.  \nThis anchors the model in correct factual behavior before DPO reshapes style."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\n\n# Load pairs produced by NB01 (floor strategy: score_chosen >= 0.70, delta >= 0.10)\ndf_dpo = pd.read_json(DATA_DIR / \"dpo_floor.jsonl\", lines=True)\n\n# SFT warm-up: use only the highest-scoring chosen responses\n# score_chosen was saved alongside prompt/chosen/rejected in NB01\nSFT_SCORE_FLOOR = 0.80\nif \"score_chosen\" in df_dpo.columns:\n    df_sft = df_dpo[df_dpo[\"score_chosen\"] >= SFT_SCORE_FLOOR].copy()\nelse:\n    # Fallback: top quartile by position if score not present in file\n    df_sft = df_dpo.nlargest(min(700, len(df_dpo)), \"score_chosen\") if \"score_chosen\" in df_dpo.columns \\\n             else df_dpo.sample(min(700, len(df_dpo)), random_state=42)\n\nprint(f\"SFT warm-up candidates: {len(df_sft):,}\")\n\n\ndef to_sft_text(row):\n    \"\"\"Format as chat-style text for causal LM training.\"\"\"\n    return f\"### User:\\n{row['prompt']}\\n\\n### Assistant:\\n{row['chosen']}\"\n\n\ndf_sft[\"text\"] = df_sft.apply(to_sft_text, axis=1)\nds_sft = Dataset.from_pandas(df_sft[[\"text\"]])\n\nprint(f\"SFT warm-up samples: {len(ds_sft):,}\")\nprint(ds_sft[0][\"text\"][:300])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_args = TrainingArguments(\n",
    "    output_dir=str(MODELS_DIR / f\"{RUN_NAME}-sft\"),\n",
    "    num_train_epochs=SFT_EPOCHS,\n",
    "    per_device_train_batch_size=SFT_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=SFT_LR,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=\"wandb\" if os.getenv(\"WANDB_API_KEY\") else \"none\",\n",
    "    run_name=f\"{RUN_NAME}-sft\",\n",
    ")\n",
    "\n",
    "sft_trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_args,\n",
    "    train_dataset=ds_sft,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=SFT_MAX_SEQ_LEN,\n",
    ")\n",
    "\n",
    "print(\"Starting SFT warm-up...\")\n",
    "sft_trainer.train()\n",
    "sft_trainer.save_model(str(MODELS_DIR / f\"{RUN_NAME}-sft-final\"))\n",
    "print(\"SFT complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Phase 2 — DPO with Raised β\n",
    "\n",
    "`β = 0.2` (vs. default 0.1) → stronger KL penalty → model stays closer to SFT distribution, reducing correctness decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DPO pairs\n",
    "ds_dpo = Dataset.from_pandas(df_dpo[[\"prompt\", \"chosen\", \"rejected\"]])\n",
    "split   = ds_dpo.train_test_split(test_size=0.05, seed=42)\n",
    "\n",
    "print(f\"DPO train: {len(split['train']):,}  |  eval: {len(split['test']):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_config = DPOConfig(\n",
    "    beta=DPO_BETA,\n",
    "    output_dir=str(MODELS_DIR / f\"{RUN_NAME}-dpo\"),\n",
    "    num_train_epochs=DPO_EPOCHS,\n",
    "    per_device_train_batch_size=DPO_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=DPO_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=DPO_LR,\n",
    "    bf16=True,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    max_length=DPO_MAX_LENGTH,\n",
    "    max_prompt_length=512,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=\"wandb\" if os.getenv(\"WANDB_API_KEY\") else \"none\",\n",
    "    run_name=f\"{RUN_NAME}-dpo\",\n",
    ")\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=None,   # None = use frozen SFT checkpoint as reference\n",
    "    args=dpo_config,\n",
    "    train_dataset=split[\"train\"],\n",
    "    eval_dataset=split[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(f\"Starting DPO (β={DPO_BETA})...\")\n",
    "dpo_trainer.train()\n",
    "dpo_trainer.save_model(str(MODELS_DIR / f\"{RUN_NAME}-dpo-final\"))\n",
    "print(\"DPO complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quick Inference Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "test_prompt = \"My order hasn't arrived after 2 weeks. What should I do?\"\n",
    "inputs = tokenizer(\n",
    "    f\"### User:\\n{test_prompt}\\n\\n### Assistant:\\n\",\n",
    "    return_tensors=\"pt\"\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(output[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "print(\"Prompt:\", test_prompt)\n",
    "print(\"\\nResponse:\\n\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Phase | Method | Key setting | Purpose |\n",
    "|-------|--------|------------|--------|\n",
    "| 1 | SFT warm-up | 700 high-correctness samples | Anchor factual quality |\n",
    "| 2 | DPO | β=0.2 (raised KL) | Style alignment without correctness decay |\n",
    "\n",
    "**Next:** `03_model_evaluation.ipynb` — measure correctness, groundedness, style before and after."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
