{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 — Notebook 4: Alignment Pain Points & Remedies\n",
    "\n",
    "## Context\n",
    "\n",
    "This notebook documents and diagnoses the real-world technical hurdles observed during preference-based alignment (DPO/ORPO) on a customer support model. Data is processed via Spark (~90k rows), scored by VJs (Virtual Judges), and used to construct chosen/rejected pairs.\n",
    "\n",
    "---\n",
    "\n",
    "## Pain Points Covered\n",
    "\n",
    "| # | Pain Point | Section |\n",
    "|---|------------|---------|\n",
    "| 1 | **Alignment Tax** — style improves, correctness decays | §2 |\n",
    "| 2 | **Evaluation Uncertainty** — unknown VJ precision & recall | §3 |\n",
    "| 3 | **Negative Correlation in Rule-Based Scoring** — rejected > chosen on problem_solution | §4 |\n",
    "| 4 | **Sample Efficiency** — 24k → 700 high-quality pairs after filtering | §5 |\n",
    "| 5 | **Reward Non-Determinism** — correctness scores feel random | §6 |\n",
    "\n",
    "## Remedies Implemented\n",
    "\n",
    "| Remedy | Concept | Section |\n",
    "|--------|---------|--------|\n",
    "| Multi-Objective Optimization (MOO) | Correctness floor constraint | §7 |\n",
    "| Raised KL Penalty (β) | Reference model regularization | §8 |\n",
    "| Judge-on-Judge Meta-Eval | Estimating VJ precision | §9 |\n",
    "| SFT Warm-up → DPO | Differentiated training phases | §10 |\n",
    "| On-Policy DPO | Sample from current model, not GPT-5/Kimi | §11 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"muted\")\n",
    "EVAL_DIR = Path(\"../data/eval_results\")\n",
    "EVAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# ── Synthetic data to illustrate each pain point ─────────────────────────────\n",
    "# Replace with real df_scores from Notebook 03 if available:\n",
    "# df_scores = pd.read_json(EVAL_DIR / \"vj_scores.jsonl\", lines=True)\n",
    "\n",
    "n = 200\n",
    "rng = np.random.default_rng(42)\n",
    "df_scores = pd.concat([\n",
    "    pd.DataFrame({\n",
    "        \"checkpoint\":      \"base\",\n",
    "        \"correctness\":     rng.normal(0.72, 0.10, n).clip(0,1),\n",
    "        \"groundedness\":    rng.normal(0.70, 0.10, n).clip(0,1),\n",
    "        \"problem_solution\":rng.normal(0.68, 0.12, n).clip(0,1),\n",
    "        \"style\":           rng.normal(0.55, 0.12, n).clip(0,1),\n",
    "    }),\n",
    "    pd.DataFrame({\n",
    "        \"checkpoint\":      \"sft\",\n",
    "        \"correctness\":     rng.normal(0.76, 0.09, n).clip(0,1),\n",
    "        \"groundedness\":    rng.normal(0.73, 0.09, n).clip(0,1),\n",
    "        \"problem_solution\":rng.normal(0.72, 0.11, n).clip(0,1),\n",
    "        \"style\":           rng.normal(0.57, 0.11, n).clip(0,1),\n",
    "    }),\n",
    "    pd.DataFrame({\n",
    "        \"checkpoint\":      \"dpo_naive\",  # no correctness floor, low β\n",
    "        \"correctness\":     rng.normal(0.64, 0.11, n).clip(0,1),   # ← decays\n",
    "        \"groundedness\":    rng.normal(0.65, 0.10, n).clip(0,1),\n",
    "        \"problem_solution\":rng.normal(0.63, 0.13, n).clip(0,1),   # ← decays\n",
    "        \"style\":           rng.normal(0.73, 0.09, n).clip(0,1),   # ← improves\n",
    "    }),\n",
    "    pd.DataFrame({\n",
    "        \"checkpoint\":      \"dpo_fixed\",  # correctness floor + β=0.2\n",
    "        \"correctness\":     rng.normal(0.74, 0.09, n).clip(0,1),\n",
    "        \"groundedness\":    rng.normal(0.72, 0.09, n).clip(0,1),\n",
    "        \"problem_solution\":rng.normal(0.71, 0.11, n).clip(0,1),\n",
    "        \"style\":           rng.normal(0.70, 0.09, n).clip(0,1),\n",
    "    }),\n",
    "], ignore_index=True)\n",
    "\n",
    "DIMS = [\"correctness\", \"groundedness\", \"problem_solution\", \"style\"]\n",
    "print(df_scores.groupby(\"checkpoint\")[DIMS].mean().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pain Point 1 — The Alignment Tax\n",
    "\n",
    "> When optimizing for style/tone, we observe simultaneous decay in correctness and problem_solution.  \n",
    "> This is **not** a model failure — it is a **data construction failure**: the chosen/rejected pairs did not  \n",
    "> hold correctness constant while varying style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4), sharey=False)\n",
    "order = [\"base\", \"sft\", \"dpo_naive\", \"dpo_fixed\"]\n",
    "colors = [\"#4878CF\", \"#6ACC65\", \"#D65F5F\", \"#B47CC7\"]\n",
    "\n",
    "for ax, dim in zip(axes, DIMS):\n",
    "    means = df_scores.groupby(\"checkpoint\")[dim].mean().reindex(order)\n",
    "    cis   = df_scores.groupby(\"checkpoint\")[dim].sem().reindex(order) * 1.96\n",
    "    ax.bar(range(len(order)), means, color=colors, yerr=cis, capsize=4, width=0.6)\n",
    "    ax.set_xticks(range(len(order)))\n",
    "    ax.set_xticklabels(order, rotation=25, ha=\"right\", fontsize=9)\n",
    "    ax.set_title(dim, fontsize=12)\n",
    "    ax.set_ylim(0.4, 0.9)\n",
    "    for i, v in enumerate(means):\n",
    "        ax.text(i, v + cis.iloc[i] + 0.01, f\"{v:.2f}\", ha=\"center\", fontsize=8)\n",
    "\n",
    "plt.suptitle(\"Pain Point 1: Alignment Tax\\ndpo_naive shows style↑ but correctness↓\",\n",
    "             fontsize=13, y=1.03)\n",
    "plt.tight_layout()\n",
    "plt.savefig(EVAL_DIR / \"pain1_alignment_tax.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pain Point 2 — Evaluation Uncertainty\n",
    "\n",
    "> We do not know the precision and recall of our VJ scoring pipeline.  \n",
    "> Without calibration, a high score may mean nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate VJ (weak judge) vs. strong judge scores for 100 samples\n",
    "n_meta = 100\n",
    "true_scores   = rng.uniform(0.3, 1.0, n_meta)\n",
    "\n",
    "# VJ with random noise (simulates non-determinism in correctness scoring)\n",
    "vj_scores     = (true_scores + rng.normal(0, 0.15, n_meta)).clip(0, 1)\n",
    "\n",
    "# Bucketise to high/low\n",
    "THRESHOLD = 0.65\n",
    "true_positive  = ((vj_scores >= THRESHOLD) & (true_scores >= THRESHOLD)).sum()\n",
    "false_positive = ((vj_scores >= THRESHOLD) & (true_scores <  THRESHOLD)).sum()\n",
    "false_negative = ((vj_scores <  THRESHOLD) & (true_scores >= THRESHOLD)).sum()\n",
    "true_negative  = ((vj_scores <  THRESHOLD) & (true_scores <  THRESHOLD)).sum()\n",
    "\n",
    "precision = true_positive / (true_positive + false_positive + 1e-9)\n",
    "recall    = true_positive / (true_positive + false_negative + 1e-9)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.scatter(true_scores, vj_scores, alpha=0.5, s=30)\n",
    "ax.axvline(THRESHOLD, color=\"red\",  linestyle=\"--\", label=\"threshold (true)\")\n",
    "ax.axhline(THRESHOLD, color=\"blue\", linestyle=\"--\", label=\"threshold (VJ)\")\n",
    "ax.set_xlabel(\"Strong judge score (ground truth)\")\n",
    "ax.set_ylabel(\"VJ score\")\n",
    "ax.set_title(f\"Pain Point 2: VJ Calibration\\nPrecision={precision:.2f}  Recall={recall:.2f}\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(EVAL_DIR / \"pain2_evaluation_uncertainty.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall:    {recall:.2f}\")\n",
    "print(\"\\n→ Remedy (§9): Judge-on-Judge meta-eval on 500 samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pain Point 3 — Negative Correlation in Rule-Based Scoring\n",
    "\n",
    "> When rule-based scores select 5,000 pairs, the **rejected** group averages *higher*  \n",
    "> problem_solution than the chosen group — a structural inversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the inversion\n",
    "n_pairs = 5000\n",
    "\n",
    "# Rule selects on style delta alone — problem_solution is NOT controlled\n",
    "df_pairs = pd.DataFrame({\n",
    "    \"style_chosen\":            rng.normal(0.72, 0.10, n_pairs).clip(0,1),\n",
    "    \"style_rejected\":          rng.normal(0.48, 0.12, n_pairs).clip(0,1),\n",
    "    # problem_solution negatively correlated with style in this dataset\n",
    "    \"ps_chosen\":               rng.normal(0.58, 0.15, n_pairs).clip(0,1),\n",
    "    \"ps_rejected\":             rng.normal(0.66, 0.14, n_pairs).clip(0,1),  # ← perversely higher\n",
    "})\n",
    "\n",
    "ps_c = df_pairs[\"ps_chosen\"].mean()\n",
    "ps_r = df_pairs[\"ps_rejected\"].mean()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.bar([\"chosen\", \"rejected\"], [ps_c, ps_r], color=[\"steelblue\", \"tomato\"], width=0.4)\n",
    "ax.set_ylabel(\"Avg problem_solution score\")\n",
    "ax.set_title(f\"Pain Point 3: Negative Correlation in Rule-Based Selection\\n\"\n",
    "             f\"rejected ({ps_r:.3f}) > chosen ({ps_c:.3f}) on problem_solution\")\n",
    "ax.set_ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.savefig(EVAL_DIR / \"pain3_negative_correlation.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"→ Remedy (§7): Add problem_solution floor constraint to pair selection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pain Point 4 — Sample Efficiency\n",
    "\n",
    "> Aggressive multi-VJ filtering collapses 24k pairs → ~700 high-quality pairs.  \n",
    "> Each additional filter cuts yield multiplicatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funnel = pd.DataFrame([\n",
    "    {\"stage\": \"Raw rollouts\",                  \"n\": 90_000},\n",
    "    {\"stage\": \"Deduplication + quality\",        \"n\": 24_000},\n",
    "    {\"stage\": \"Reward delta ≥ 0.05\",            \"n\": 8_500},\n",
    "    {\"stage\": \"Correctness floor ≥ 0.60\",       \"n\": 3_200},\n",
    "    {\"stage\": \"Groundedness floor ≥ 0.55\",      \"n\": 1_400},\n",
    "    {\"stage\": \"Problem_solution floor ≥ 0.60\",  \"n\":   700},\n",
    "])\n",
    "funnel[\"retention_%\"] = (funnel[\"n\"] / funnel[\"n\"].iloc[0] * 100).round(2)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 4))\n",
    "bars = ax.barh(funnel[\"stage\"][::-1], funnel[\"n\"][::-1], color=sns.color_palette(\"Blues_r\", len(funnel)))\n",
    "for bar, row in zip(bars, funnel[::-1].itertuples()):\n",
    "    ax.text(bar.get_width() + 200, bar.get_y() + bar.get_height()/2,\n",
    "            f\"{row.n:,}  ({row._4:.1f}%)\", va=\"center\", fontsize=9)\n",
    "ax.set_xlabel(\"Number of pairs\")\n",
    "ax.set_title(\"Pain Point 4: Sample Efficiency Funnel\")\n",
    "ax.set_xlim(0, 100_000)\n",
    "plt.tight_layout()\n",
    "plt.savefig(EVAL_DIR / \"pain4_sample_efficiency.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(funnel.to_string(index=False))\n",
    "print(\"\\n→ Remedy (§11): On-policy sampling produces more filterable signal per rollout.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pain Point 5 — Reward Non-Determinism\n",
    "\n",
    "> Correctness scoring feels random — the same response can score 0.55 or 0.78  \n",
    "> across two VJ calls. This erodes trust in the chosen/rejected delta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 300\n",
    "true_quality = rng.uniform(0.3, 1.0, n_samples)\n",
    "# Two independent VJ runs with noise\n",
    "run1 = (true_quality + rng.normal(0, 0.12, n_samples)).clip(0, 1)\n",
    "run2 = (true_quality + rng.normal(0, 0.12, n_samples)).clip(0, 1)\n",
    "score_variance = np.abs(run1 - run2)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.scatter(run1, run2, alpha=0.4, s=20)\n",
    "ax1.plot([0,1],[0,1], \"r--\", linewidth=1)\n",
    "r, _ = stats.pearsonr(run1, run2)\n",
    "ax1.set_xlabel(\"VJ Run 1\")\n",
    "ax1.set_ylabel(\"VJ Run 2\")\n",
    "ax1.set_title(f\"Correctness Score Agreement (r={r:.3f})\")\n",
    "\n",
    "ax2.hist(score_variance, bins=30, color=\"salmon\", edgecolor=\"white\")\n",
    "ax2.axvline(score_variance.mean(), color=\"darkred\", linestyle=\"--\",\n",
    "            label=f\"mean |Δ| = {score_variance.mean():.3f}\")\n",
    "ax2.set_xlabel(\"|Score run1 - run2|\")\n",
    "ax2.set_title(\"Pain Point 5: Score Non-Determinism\")\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(EVAL_DIR / \"pain5_nondeterminism.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean absolute score variance across two VJ runs: {score_variance.mean():.3f}\")\n",
    "print(\"→ Remedy: Temperature=0 for VJ calls + ensemble scoring (avg 3 runs).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Remedy 1 — Multi-Objective Optimization: Correctness Floor\n",
    "\n",
    "> Instead of a simple weighted sum, use **constrained selection**:  \n",
    "> a pair is only included if the *chosen* response passes a minimum correctness threshold,  \n",
    "> regardless of how large the style delta is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5000\n",
    "df_moo = pd.DataFrame({\n",
    "    \"correctness_chosen\":      rng.normal(0.65, 0.15, n).clip(0, 1),\n",
    "    \"style_delta\":             rng.normal(0.15, 0.10, n).clip(0, 0.5),\n",
    "    \"problem_solution_chosen\": rng.normal(0.62, 0.14, n).clip(0, 1),\n",
    "})\n",
    "\n",
    "# Weighted sum (no constraint)\n",
    "df_unconstrained = df_moo[df_moo[\"style_delta\"] > 0.10]\n",
    "\n",
    "# MOO with correctness floor\n",
    "CORRECTNESS_FLOOR = 0.60\n",
    "df_constrained = df_moo[\n",
    "    (df_moo[\"style_delta\"] > 0.10)\n",
    "    & (df_moo[\"correctness_chosen\"] >= CORRECTNESS_FLOOR)\n",
    "]\n",
    "\n",
    "compare = pd.DataFrame({\n",
    "    \"pairs\": [len(df_unconstrained), len(df_constrained)],\n",
    "    \"avg_correctness\": [\n",
    "        df_unconstrained[\"correctness_chosen\"].mean(),\n",
    "        df_constrained[\"correctness_chosen\"].mean(),\n",
    "    ],\n",
    "    \"avg_problem_solution\": [\n",
    "        df_unconstrained[\"problem_solution_chosen\"].mean(),\n",
    "        df_constrained[\"problem_solution_chosen\"].mean(),\n",
    "    ],\n",
    "}, index=[\"unconstrained\", \"MOO (floor)\"])\n",
    "\n",
    "print(compare.round(3))\n",
    "print(f\"\\nPairs removed by floor: {len(df_unconstrained) - len(df_constrained):,}\")\n",
    "print(f\"Correctness improvement: {compare.loc['MOO (floor)','avg_correctness'] - compare.loc['unconstrained','avg_correctness']:+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Remedy 2 — Raised KL Penalty (β) in DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise how β controls trade-off between reward and KL divergence\n",
    "betas = [0.05, 0.10, 0.15, 0.20, 0.30, 0.50]\n",
    "# Hypothetical: higher β → less style gain but more correctness preserved\n",
    "style_gain       = [0.18, 0.15, 0.13, 0.10, 0.07, 0.04]\n",
    "correctness_loss = [0.12, 0.08, 0.05, 0.02, 0.01, 0.005]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "ax.plot(betas, style_gain,       \"o-\", color=\"steelblue\", label=\"Style gain\")\n",
    "ax.plot(betas, correctness_loss, \"s-\", color=\"tomato\",    label=\"Correctness loss\")\n",
    "ax.axvline(0.20, linestyle=\"--\", color=\"green\", label=\"Selected β=0.20\")\n",
    "ax.set_xlabel(\"DPO β (KL penalty)\")\n",
    "ax.set_ylabel(\"Score delta from base\")\n",
    "ax.set_title(\"Remedy 2: β Trade-off — More KL = Less Tax\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(EVAL_DIR / \"remedy2_beta_tradeoff.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Selected β=0.20 as the knee point: meaningful style improvement with minimal correctness decay.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Remedy 3 — Judge-on-Judge Meta-Eval\n",
    "\n",
    "See `03_model_evaluation.ipynb §5` for the live implementation.  \n",
    "Here we summarize the framework and interpret the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret agreement scores\n",
    "agreement_results = {\n",
    "    \"correctness\":      {\"r\": 0.81, \"interpretation\": \"good — reliable signal\"},\n",
    "    \"groundedness\":     {\"r\": 0.74, \"interpretation\": \"good\"},\n",
    "    \"problem_solution\": {\"r\": 0.62, \"interpretation\": \"moderate — use with caution\"},\n",
    "    \"style\":            {\"r\": 0.53, \"interpretation\": \"⚠ low — subjective, high noise\"},\n",
    "}\n",
    "\n",
    "df_agree = pd.DataFrame(agreement_results).T.reset_index()\n",
    "df_agree.columns = [\"dimension\", \"pearson_r\", \"interpretation\"]\n",
    "df_agree[\"pearson_r\"] = df_agree[\"pearson_r\"].astype(float)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 3))\n",
    "colors = [\"green\" if r >= 0.7 else \"orange\" if r >= 0.5 else \"red\"\n",
    "          for r in df_agree[\"pearson_r\"]]\n",
    "ax.barh(df_agree[\"dimension\"], df_agree[\"pearson_r\"], color=colors)\n",
    "ax.axvline(0.70, linestyle=\"--\", color=\"green\",  label=\"Good (r≥0.70)\")\n",
    "ax.axvline(0.50, linestyle=\"--\", color=\"orange\", label=\"Moderate (r≥0.50)\")\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_xlabel(\"Pearson r (VJ vs. GPT-4o)\")\n",
    "ax.set_title(\"Remedy 3: Judge Agreement (Meta-Eval)\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(EVAL_DIR / \"remedy3_judge_agreement.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "for _, row in df_agree.iterrows():\n",
    "    print(f\"  {row['dimension']:<22} r={row['pearson_r']:.2f}  [{row['interpretation']}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Remedy 4 — SFT Warm-up Strategy\n",
    "\n",
    "See `02_finetuning_qlora.ipynb` for implementation.  \n",
    "This cell explains *why* it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual: training trajectory — correctness over training steps\n",
    "steps = np.arange(0, 500, 10)\n",
    "\n",
    "# Without SFT warm-up: correctness drops as DPO optimizes style\n",
    "correctness_no_warmup = 0.72 - 0.10 * (1 - np.exp(-steps / 400)) + rng.normal(0, 0.01, len(steps))\n",
    "\n",
    "# With SFT warm-up: starts higher, decays less\n",
    "correctness_warmup = 0.76 - 0.04 * (1 - np.exp(-steps / 400)) + rng.normal(0, 0.01, len(steps))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(steps, correctness_no_warmup, color=\"tomato\",    label=\"DPO only (no warm-up)\")\n",
    "ax.plot(steps, correctness_warmup,    color=\"steelblue\", label=\"SFT warm-up → DPO\")\n",
    "ax.axvspan(0, 50, alpha=0.1, color=\"green\", label=\"SFT warm-up phase\")\n",
    "ax.set_xlabel(\"DPO training steps\")\n",
    "ax.set_ylabel(\"Correctness score\")\n",
    "ax.set_title(\"Remedy 4: SFT Warm-up Anchors Correctness During DPO\")\n",
    "ax.legend()\n",
    "ax.set_ylim(0.5, 0.9)\n",
    "plt.tight_layout()\n",
    "plt.savefig(EVAL_DIR / \"remedy4_sft_warmup.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Remedy 5 — On-Policy DPO\n",
    "\n",
    "> **Problem:** Using off-policy data from GPT-5/Kimi as ground truth means the distribution  \n",
    "> gap between the reference responses and the model's actual outputs is large.  \n",
    "> The model is being trained on errors it doesn't even make.\n",
    ">\n",
    "> **Fix:** Sample rollouts from the *current* checkpoint of the Qwen model → score them → use for next DPO round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudo-code / workflow for iterative on-policy DPO\n",
    "\n",
    "ON_POLICY_WORKFLOW = \"\"\"\n",
    "Iterative On-Policy DPO Loop\n",
    "════════════════════════════\n",
    "\n",
    "Round 0:\n",
    "  model_0 = Qwen2.5-7B-Instruct (base)\n",
    "\n",
    "For round t = 1, 2, 3, ...:\n",
    "  1. Sample K rollouts from model_{t-1} for each prompt in pool\n",
    "     └── temperature=0.7, top_p=0.9\n",
    "  2. Score rollouts with VJ (correctness, groundedness, ps, style)\n",
    "  3. Construct pairs:\n",
    "     └── chosen   = rollout with highest weighted reward\n",
    "     └── rejected = rollout with lowest weighted reward\n",
    "     └── Apply correctness floor ≥ 0.60 (Remedy 1)\n",
    "  4. Train DPO (β=0.20) for 1 epoch → model_t\n",
    "  5. Evaluate model_t on held-out set\n",
    "     └── Early stop if correctness decays > 2% vs. model_{t-1}\n",
    "\n",
    "Advantages:\n",
    "  - Data targets the model's actual failure modes\n",
    "  - Distribution gap is minimized (on-policy)\n",
    "  - Sample efficiency improves each round\n",
    "\"\"\"\n",
    "\n",
    "print(ON_POLICY_WORKFLOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise expected improvement curve under on-policy vs. off-policy DPO\n",
    "rounds = np.arange(0, 5)\n",
    "\n",
    "style_offpolicy = [0.55, 0.65, 0.68, 0.69, 0.70]\n",
    "style_onpolicy  = [0.55, 0.67, 0.72, 0.74, 0.76]\n",
    "correct_offpolicy = [0.72, 0.65, 0.63, 0.62, 0.61]\n",
    "correct_onpolicy  = [0.72, 0.70, 0.70, 0.71, 0.72]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(rounds, style_offpolicy, \"o--\", color=\"tomato\",    label=\"Off-policy (GPT-5/Kimi)\")\n",
    "ax1.plot(rounds, style_onpolicy,  \"s-\",  color=\"steelblue\", label=\"On-policy (Qwen rollouts)\")\n",
    "ax1.set_title(\"Style Score per Round\")\n",
    "ax1.set_xlabel(\"DPO Round\")\n",
    "ax1.set_ylabel(\"Avg style score\")\n",
    "ax1.set_ylim(0.4, 0.9)\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(rounds, correct_offpolicy, \"o--\", color=\"tomato\",    label=\"Off-policy\")\n",
    "ax2.plot(rounds, correct_onpolicy,  \"s-\",  color=\"steelblue\", label=\"On-policy\")\n",
    "ax2.set_title(\"Correctness Score per Round\")\n",
    "ax2.set_xlabel(\"DPO Round\")\n",
    "ax2.set_ylabel(\"Avg correctness score\")\n",
    "ax2.set_ylim(0.4, 0.9)\n",
    "ax2.legend()\n",
    "\n",
    "plt.suptitle(\"Remedy 5: On-Policy DPO Preserves Correctness While Improving Style\",\n",
    "             fontsize=13, y=1.03)\n",
    "plt.tight_layout()\n",
    "plt.savefig(EVAL_DIR / \"remedy5_onpolicy_dpo.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Pain Points → Remedies Mapping\n",
    "\n",
    "| Pain Point | Root Cause | Remedy | Notebook |\n",
    "|------------|-----------|--------|----------|\n",
    "| Alignment tax | Pairs don't hold correctness constant | MOO correctness floor | §7 / NB01 |\n",
    "| Evaluation uncertainty | VJ not calibrated | Judge-on-Judge meta-eval | §9 / NB03 |\n",
    "| Negative correlation | Rule-based selection ignores cross-metric effects | Multi-floor pair selection | §7 / NB01 |\n",
    "| Sample collapse | Over-filtering across multiple VJs | Relax individual floors; prioritize on-policy data | §11 |\n",
    "| Score non-determinism | LLM judge temperature > 0 | Set VJ temperature=0, ensemble 3 runs | §6 |\n",
    "| Metric decay during DPO | KL penalty too weak | Raise β from 0.10 → 0.20 | §8 / NB02 |\n",
    "| Off-policy gap | Training on GPT-5/Kimi errors, not model's errors | Iterative on-policy DPO | §11 |\n",
    "\n",
    "---\n",
    "> **Artifacts:** All charts saved to `../data/eval_results/`  \n",
    "> **Next step:** Integrate remedies into NB01 pair construction and re-run NB02 → NB03 to verify."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
