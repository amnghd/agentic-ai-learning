{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Week 2 — Notebook 1: Dataset Preparation & DPO Pair Construction\n\nThis notebook covers:\n1. Loading and inspecting raw data (~90k rows)\n2. De-identification with standard libraries (no LLMs)\n3. Data quality assessment\n4. Extracting existing chosen/rejected pairs **with their pre-computed scores**\n5. Two filtering strategies using those real scores:\n   - **Score delta threshold** (keep pairs where chosen >> rejected)\n   - **Score floor + delta** (additionally require chosen to clear a minimum quality bar)\n6. Sample efficiency analysis\n\n---\n> **GPU requirement:** None — CPU/data-engineering only.  \n> **Estimated runtime:** ~10–20 min depending on dataset size."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Environment & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed (comment out after first run)\n",
    "# !pip install datasets pandas pyarrow presidio-analyzer presidio-anonymizer spacy tqdm\n",
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path(\"../data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Imports OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Raw Dataset\n",
    "\n",
    "We use the **Argilla Distilabel Customer Support** dataset as a realistic proxy.  \n",
    "Replace `DATASET_PATH` with your own Spark-processed parquet/JSONL if you have it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Option A: Load from HuggingFace Hub (public proxy dataset) ---\n",
    "dataset = load_dataset(\"HuggingFaceH4/ultrafeedback_binarized\", split=\"train_prefs\")\n",
    "df = dataset.to_pandas()\n",
    "\n",
    "# --- Option B: Load your own data ---\n",
    "# df = pd.read_parquet(DATA_DIR / \"rollouts_raw.parquet\")\n",
    "# df = pd.read_json(DATA_DIR / \"rollouts_raw.jsonl\", lines=True)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick stats\n",
    "print(df.dtypes)\n",
    "print(\"\\nNull counts:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. De-identification (Standard Libraries)\n",
    "\n",
    "Using **Microsoft Presidio** — no LLM calls, fully deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from presidio_analyzer import AnalyzerEngine\n",
    "    from presidio_anonymizer import AnonymizerEngine\n",
    "\n",
    "    analyzer = AnalyzerEngine()\n",
    "    anonymizer = AnonymizerEngine()\n",
    "    PRESIDIO_AVAILABLE = True\n",
    "    print(\"Presidio loaded.\")\n",
    "except ImportError:\n",
    "    PRESIDIO_AVAILABLE = False\n",
    "    print(\"Presidio not installed — skipping de-id (install presidio-analyzer presidio-anonymizer spacy + en_core_web_lg)\")\n",
    "\n",
    "\n",
    "def deidentify(text: str) -> str:\n",
    "    \"\"\"Replace PII entities with type placeholders, e.g. <PERSON>.\"\"\"\n",
    "    if not PRESIDIO_AVAILABLE or not isinstance(text, str):\n",
    "        return text\n",
    "    results = analyzer.analyze(text=text, language=\"en\")\n",
    "    anonymized = anonymizer.anonymize(text=text, analyzer_results=results)\n",
    "    return anonymized.text\n",
    "\n",
    "\n",
    "# Test\n",
    "sample = \"Hi, I'm John Smith and my email is john@example.com, phone 555-1234.\"\n",
    "print(\"Before:\", sample)\n",
    "print(\"After: \", deidentify(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to the prompt column (adjust column name to your schema)\n",
    "PROMPT_COL = \"prompt\"  # change if your column is named differently\n",
    "\n",
    "if PROMPT_COL in df.columns and PRESIDIO_AVAILABLE:\n",
    "    tqdm.pandas(desc=\"De-identifying prompts\")\n",
    "    df[\"prompt_clean\"] = df[PROMPT_COL].progress_apply(deidentify)\n",
    "    changed = (df[\"prompt_clean\"] != df[PROMPT_COL]).sum()\n",
    "    print(f\"Records modified by de-id: {changed}/{len(df)} ({changed/len(df)*100:.1f}%)\")\n",
    "else:\n",
    "    df[\"prompt_clean\"] = df.get(PROMPT_COL, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality_metrics(df: pd.DataFrame, text_col: str) -> pd.DataFrame:\n",
    "    \"\"\"Compute basic quality signals on a text column.\"\"\"\n",
    "    d = df.copy()\n",
    "    d[\"len_chars\"] = d[text_col].str.len()\n",
    "    d[\"len_words\"] = d[text_col].str.split().str.len()\n",
    "    d[\"is_empty\"] = d[text_col].str.strip().str.len() == 0\n",
    "    d[\"has_url\"] = d[text_col].str.contains(r\"https?://\", na=False)\n",
    "    d[\"is_duplicate\"] = d.duplicated(subset=[text_col], keep=False)\n",
    "    return d\n",
    "\n",
    "\n",
    "df = quality_metrics(df, \"prompt_clean\")\n",
    "\n",
    "print(\"=== Quality Summary ===\")\n",
    "print(f\"Total rows        : {len(df):,}\")\n",
    "print(f\"Empty prompts     : {df['is_empty'].sum():,}\")\n",
    "print(f\"Duplicate prompts : {df['is_duplicate'].sum():,}\")\n",
    "print(f\"Contains URL      : {df['has_url'].sum():,}\")\n",
    "print(f\"\\nPrompt length (words):\")\n",
    "print(df[\"len_words\"].describe().round(1).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out bad rows\n",
    "before = len(df)\n",
    "df = df[\n",
    "    ~df[\"is_empty\"]\n",
    "    & ~df.duplicated(subset=[\"prompt_clean\"], keep=\"first\")\n",
    "    & df[\"len_words\"].between(5, 2000)\n",
    "].reset_index(drop=True)\n",
    "print(f\"Rows after quality filter: {len(df):,} (removed {before - len(df):,})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Extract Chosen / Rejected Text and Their Pre-Computed Scores\n\nThe dataset already ships with `score_chosen` and `score_rejected` — no VJ needed here."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def extract_last_assistant(messages) -> str:\n    \"\"\"Pull the final assistant turn from a list-of-dicts conversation.\"\"\"\n    if isinstance(messages, list):\n        for msg in reversed(messages):\n            if isinstance(msg, dict) and msg.get(\"role\") == \"assistant\":\n                return msg.get(\"content\", \"\")\n    return str(messages)\n\n\n# Extract text\ndf[\"response_chosen\"]   = df[\"chosen\"].apply(extract_last_assistant)\ndf[\"response_rejected\"] = df[\"rejected\"].apply(extract_last_assistant)\n\n# Scores are already floats in the dataset\n# UltraFeedback uses a 1–5 scale; normalise to [0, 1]\nSCORE_MAX = 5.0\ndf[\"score_chosen\"]   = pd.to_numeric(df[\"score_chosen\"],   errors=\"coerce\") / SCORE_MAX\ndf[\"score_rejected\"] = pd.to_numeric(df[\"score_rejected\"], errors=\"coerce\") / SCORE_MAX\n\n# Drop rows where scores are missing\nbefore = len(df)\ndf = df.dropna(subset=[\"score_chosen\", \"score_rejected\"]).reset_index(drop=True)\nprint(f\"Rows with valid scores: {len(df):,} (dropped {before - len(df):,} missing-score rows)\")\n\nprint(\"\\nScore distributions (normalised 0–1):\")\nprint(df[[\"score_chosen\", \"score_rejected\"]].describe().round(3))\n\ndf[\"score_delta\"] = df[\"score_chosen\"] - df[\"score_rejected\"]\nprint(f\"\\nscore_delta stats:\\n{df['score_delta'].describe().round(3)}\")\nprint(f\"Pairs where chosen > rejected: {(df['score_delta'] > 0).mean()*100:.1f}%\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Strategy A — Score Delta Threshold\n\nKeep pairs where `score_chosen - score_rejected ≥ DELTA_THRESHOLD`.  \nA tight threshold gives cleaner signal but fewer pairs (see §7 for the funnel)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "DELTA_THRESHOLD = 0.10   # ~0.5 points on the original 1–5 scale\n\ndf_delta = df[df[\"score_delta\"] >= DELTA_THRESHOLD].copy()\n\nprint(f\"Pairs surviving delta >= {DELTA_THRESHOLD} filter:\")\nprint(f\"  {len(df_delta):,} / {len(df):,} ({len(df_delta)/len(df)*100:.1f}%)\")\nprint(df_delta[\"score_delta\"].describe().round(3))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Strategy B — Score Floor + Delta\n\nAlso require the chosen response to clear a **minimum quality floor**.  \nThis prevents pairs where both responses are low-quality but one is marginally better."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "CHOSEN_FLOOR  = 0.70   # chosen must score >= 3.5/5 normalised\nDELTA_MIN     = 0.15   # beat rejected by >= 0.75 pts on original scale\nMAX_PAIRS     = 1000   # cap for training efficiency; set None to keep all\n\ndf_floor = df[\n    (df[\"score_chosen\"] >= CHOSEN_FLOOR)\n    & (df[\"score_delta\"] >= DELTA_MIN)\n].copy()\n\nprint(f\"Pairs surviving floor + delta filter: {len(df_floor):,} / {len(df):,} ({len(df_floor)/len(df)*100:.1f}%)\")\n\n# Cap to MAX_PAIRS, sampling randomly to avoid positional bias\nif MAX_PAIRS and len(df_floor) > MAX_PAIRS:\n    df_floor = df_floor.sample(MAX_PAIRS, random_state=42).reset_index(drop=True)\n    print(f\"Sampled down to {MAX_PAIRS} pairs (random_state=42)\")\n\nprint(f\"\\nFinal pairs     : {len(df_floor):,}\")\nprint(f\"Chosen avg score : {df_floor['score_chosen'].mean():.3f}\")\nprint(f\"Rejected avg score: {df_floor['score_rejected'].mean():.3f}\")\nprint(f\"Mean delta        : {df_floor['score_delta'].mean():.3f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sample Efficiency Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "funnel = pd.DataFrame([\n    {\"stage\": \"Raw data\",                           \"rows\": len(dataset)},\n    {\"stage\": \"After quality filter\",               \"rows\": len(df)},\n    {\"stage\": \"Strategy A (delta >= 0.10)\",         \"rows\": len(df_delta)},\n    {\"stage\": \"Strategy B (floor + delta)\",         \"rows\": len(df[\n        (df[\"score_chosen\"] >= CHOSEN_FLOOR) & (df[\"score_delta\"] >= DELTA_MIN)\n    ])},\n    {\"stage\": f\"Strategy B capped @ {MAX_PAIRS}\",   \"rows\": len(df_floor)},\n])\nfunnel[\"retention_%\"] = (funnel[\"rows\"] / funnel[\"rows\"].iloc[0] * 100).round(1)\nprint(funnel.to_string(index=False))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export DPO-Ready Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def to_dpo_format(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Convert to TRL DPO trainer schema, keeping score_chosen for SFT filtering in NB02.\"\"\"\n    return pd.DataFrame({\n        \"prompt\":        df[\"prompt_clean\"],\n        \"chosen\":        df[\"response_chosen\"],\n        \"rejected\":      df[\"response_rejected\"],\n        \"score_chosen\":  df[\"score_chosen\"],\n        \"score_rejected\":df[\"score_rejected\"],\n        \"score_delta\":   df[\"score_delta\"],\n    })\n\n\nfor name, subset in [(\"delta\", df_delta), (\"floor\", df_floor)]:\n    out = to_dpo_format(subset)\n    path = DATA_DIR / f\"dpo_{name}.jsonl\"\n    out.to_json(path, orient=\"records\", lines=True)\n    print(f\"Saved {len(out):,} pairs → {path}\")\n\n# Also save as HuggingFace Dataset (used by NB02)\nds_floor = Dataset.from_pandas(to_dpo_format(df_floor))\nds_floor.save_to_disk(str(DATA_DIR / \"dpo_floor_hf\"))\nprint(\"HuggingFace dataset saved.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Summary\n\n| Dataset | Pairs | Filter |\n|---------|-------|--------|\n| `dpo_delta.jsonl` | ~N | score_delta ≥ 0.10 |\n| `dpo_floor.jsonl` | ~N | score_chosen ≥ 0.70 AND delta ≥ 0.10 |\n\nBoth files follow TRL DPO trainer schema: `{prompt, chosen, rejected}`.\n\n**Next:** `02_finetuning_qlora.ipynb` — SFT warm-up on high-score chosen responses → DPO."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
