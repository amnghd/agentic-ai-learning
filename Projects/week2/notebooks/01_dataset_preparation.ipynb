{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 — Notebook 1: Dataset Preparation & DPO Pair Construction\n",
    "\n",
    "This notebook covers:\n",
    "1. Loading and inspecting raw data (~90k rows)\n",
    "2. De-identification with standard libraries (no LLMs)\n",
    "3. Data quality assessment\n",
    "4. Constructing chosen/rejected pairs for DPO via two strategies:\n",
    "   - **Weighted sum** scoring\n",
    "   - **Single-perspective filtering** (isolating one metric delta)\n",
    "5. Sample efficiency analysis — how much survives aggressive filtering\n",
    "\n",
    "---\n",
    "> **GPU requirement:** None — this is CPU/data-engineering work.  \n",
    "> **Estimated runtime:** ~10–20 min depending on dataset size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Environment & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed (comment out after first run)\n",
    "# !pip install datasets pandas pyarrow presidio-analyzer presidio-anonymizer spacy tqdm\n",
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path(\"../data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Imports OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Raw Dataset\n",
    "\n",
    "We use the **Argilla Distilabel Customer Support** dataset as a realistic proxy.  \n",
    "Replace `DATASET_PATH` with your own Spark-processed parquet/JSONL if you have it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Option A: Load from HuggingFace Hub (public proxy dataset) ---\n",
    "dataset = load_dataset(\"HuggingFaceH4/ultrafeedback_binarized\", split=\"train_prefs\")\n",
    "df = dataset.to_pandas()\n",
    "\n",
    "# --- Option B: Load your own data ---\n",
    "# df = pd.read_parquet(DATA_DIR / \"rollouts_raw.parquet\")\n",
    "# df = pd.read_json(DATA_DIR / \"rollouts_raw.jsonl\", lines=True)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick stats\n",
    "print(df.dtypes)\n",
    "print(\"\\nNull counts:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. De-identification (Standard Libraries)\n",
    "\n",
    "Using **Microsoft Presidio** — no LLM calls, fully deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from presidio_analyzer import AnalyzerEngine\n",
    "    from presidio_anonymizer import AnonymizerEngine\n",
    "\n",
    "    analyzer = AnalyzerEngine()\n",
    "    anonymizer = AnonymizerEngine()\n",
    "    PRESIDIO_AVAILABLE = True\n",
    "    print(\"Presidio loaded.\")\n",
    "except ImportError:\n",
    "    PRESIDIO_AVAILABLE = False\n",
    "    print(\"Presidio not installed — skipping de-id (install presidio-analyzer presidio-anonymizer spacy + en_core_web_lg)\")\n",
    "\n",
    "\n",
    "def deidentify(text: str) -> str:\n",
    "    \"\"\"Replace PII entities with type placeholders, e.g. <PERSON>.\"\"\"\n",
    "    if not PRESIDIO_AVAILABLE or not isinstance(text, str):\n",
    "        return text\n",
    "    results = analyzer.analyze(text=text, language=\"en\")\n",
    "    anonymized = anonymizer.anonymize(text=text, analyzer_results=results)\n",
    "    return anonymized.text\n",
    "\n",
    "\n",
    "# Test\n",
    "sample = \"Hi, I'm John Smith and my email is john@example.com, phone 555-1234.\"\n",
    "print(\"Before:\", sample)\n",
    "print(\"After: \", deidentify(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to the prompt column (adjust column name to your schema)\n",
    "PROMPT_COL = \"prompt\"  # change if your column is named differently\n",
    "\n",
    "if PROMPT_COL in df.columns and PRESIDIO_AVAILABLE:\n",
    "    tqdm.pandas(desc=\"De-identifying prompts\")\n",
    "    df[\"prompt_clean\"] = df[PROMPT_COL].progress_apply(deidentify)\n",
    "    changed = (df[\"prompt_clean\"] != df[PROMPT_COL]).sum()\n",
    "    print(f\"Records modified by de-id: {changed}/{len(df)} ({changed/len(df)*100:.1f}%)\")\n",
    "else:\n",
    "    df[\"prompt_clean\"] = df.get(PROMPT_COL, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality_metrics(df: pd.DataFrame, text_col: str) -> pd.DataFrame:\n",
    "    \"\"\"Compute basic quality signals on a text column.\"\"\"\n",
    "    d = df.copy()\n",
    "    d[\"len_chars\"] = d[text_col].str.len()\n",
    "    d[\"len_words\"] = d[text_col].str.split().str.len()\n",
    "    d[\"is_empty\"] = d[text_col].str.strip().str.len() == 0\n",
    "    d[\"has_url\"] = d[text_col].str.contains(r\"https?://\", na=False)\n",
    "    d[\"is_duplicate\"] = d.duplicated(subset=[text_col], keep=False)\n",
    "    return d\n",
    "\n",
    "\n",
    "df = quality_metrics(df, \"prompt_clean\")\n",
    "\n",
    "print(\"=== Quality Summary ===\")\n",
    "print(f\"Total rows        : {len(df):,}\")\n",
    "print(f\"Empty prompts     : {df['is_empty'].sum():,}\")\n",
    "print(f\"Duplicate prompts : {df['is_duplicate'].sum():,}\")\n",
    "print(f\"Contains URL      : {df['has_url'].sum():,}\")\n",
    "print(f\"\\nPrompt length (words):\")\n",
    "print(df[\"len_words\"].describe().round(1).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out bad rows\n",
    "before = len(df)\n",
    "df = df[\n",
    "    ~df[\"is_empty\"]\n",
    "    & ~df.duplicated(subset=[\"prompt_clean\"], keep=\"first\")\n",
    "    & df[\"len_words\"].between(5, 2000)\n",
    "].reset_index(drop=True)\n",
    "print(f\"Rows after quality filter: {len(df):,} (removed {before - len(df):,})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simulating VJ (Virtual Judge) Scores\n",
    "\n",
    "In production these come from your scoring pipeline.  \n",
    "Here we attach synthetic scores to demonstrate the pair-construction logic — swap in real columns when available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- If your dataframe already has score columns, skip this cell ---\n",
    "# Expected schema after this cell:\n",
    "#   score_correctness    : float [0,1]\n",
    "#   score_groundedness   : float [0,1]\n",
    "#   score_problem_solution: float [0,1]\n",
    "#   score_style          : float [0,1]\n",
    "#   response_chosen      : str\n",
    "#   response_rejected    : str\n",
    "\n",
    "def _extract_text(cell):\n",
    "    \"\"\"Handle UltraFeedback schema where responses are lists of dicts.\"\"\"\n",
    "    if isinstance(cell, list) and len(cell) > 0:\n",
    "        item = cell[0]\n",
    "        if isinstance(item, dict):\n",
    "            return item.get(\"content\", str(item))\n",
    "        return str(item)\n",
    "    return str(cell)\n",
    "\n",
    "\n",
    "# Extract chosen / rejected text\n",
    "if \"chosen\" in df.columns:\n",
    "    df[\"response_chosen\"]   = df[\"chosen\"].apply(_extract_text)\n",
    "    df[\"response_rejected\"] = df[\"rejected\"].apply(_extract_text)\n",
    "else:\n",
    "    df[\"response_chosen\"]   = \"placeholder chosen response\"\n",
    "    df[\"response_rejected\"] = \"placeholder rejected response\"\n",
    "\n",
    "# Simulate per-response VJ scores (replace with real scores in production)\n",
    "rng = np.random.default_rng(42)\n",
    "n = len(df)\n",
    "for col, mu_chosen, mu_rejected in [\n",
    "    (\"score_correctness\",     0.75, 0.50),\n",
    "    (\"score_groundedness\",    0.72, 0.55),\n",
    "    (\"score_problem_solution\",0.70, 0.52),\n",
    "    (\"score_style\",           0.65, 0.48),\n",
    "]:\n",
    "    df[f\"{col}_chosen\"]   = rng.normal(mu_chosen,   0.12, n).clip(0, 1)\n",
    "    df[f\"{col}_rejected\"] = rng.normal(mu_rejected, 0.15, n).clip(0, 1)\n",
    "\n",
    "print(df[[c for c in df.columns if c.startswith(\"score_\")]].describe().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Strategy A — Weighted Sum Pair Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS = {\n",
    "    \"score_correctness\":      0.40,\n",
    "    \"score_groundedness\":     0.25,\n",
    "    \"score_problem_solution\": 0.25,\n",
    "    \"score_style\":            0.10,\n",
    "}\n",
    "\n",
    "\n",
    "def weighted_reward(df: pd.DataFrame, suffix: str, weights: dict) -> pd.Series:\n",
    "    return sum(w * df[f\"{k}_{suffix}\"] for k, w in weights.items())\n",
    "\n",
    "\n",
    "df[\"reward_chosen\"]   = weighted_reward(df, \"chosen\",   WEIGHTS)\n",
    "df[\"reward_rejected\"] = weighted_reward(df, \"rejected\", WEIGHTS)\n",
    "df[\"reward_delta\"]    = df[\"reward_chosen\"] - df[\"reward_rejected\"]\n",
    "\n",
    "# Ensure chosen > rejected (filter out noise)\n",
    "DELTA_THRESHOLD = 0.05\n",
    "df_weighted = df[df[\"reward_delta\"] >= DELTA_THRESHOLD].copy()\n",
    "\n",
    "print(f\"Pairs surviving weighted-sum filter (delta >= {DELTA_THRESHOLD}):\")\n",
    "print(f\"  {len(df_weighted):,} / {len(df):,} ({len(df_weighted)/len(df)*100:.1f}%)\")\n",
    "print(df_weighted[\"reward_delta\"].describe().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Strategy B — Single-Perspective Filtering (Isolating Style Delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STYLE_DELTA_MIN    = 0.10   # chosen must score >= this better on style\n",
    "CORRECTNESS_FLOOR  = 0.60   # chosen must maintain at least this correctness\n",
    "\n",
    "df[\"style_delta\"] = (\n",
    "    df[\"score_style_chosen\"] - df[\"score_style_rejected\"]\n",
    ")\n",
    "\n",
    "df_style = df[\n",
    "    (df[\"style_delta\"] >= STYLE_DELTA_MIN)\n",
    "    & (df[\"score_correctness_chosen\"] >= CORRECTNESS_FLOOR)  # correctness floor!\n",
    "].copy()\n",
    "\n",
    "print(f\"Pairs surviving single-perspective style filter:\")\n",
    "print(f\"  {len(df_style):,} / {len(df):,} ({len(df_style)/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Sanity check: is the rejected group's problem_solution perversely higher?\n",
    "ps_chosen   = df_style[\"score_problem_solution_chosen\"].mean()\n",
    "ps_rejected = df_style[\"score_problem_solution_rejected\"].mean()\n",
    "print(f\"\\n[Sanity] problem_solution — chosen avg: {ps_chosen:.3f}, rejected avg: {ps_rejected:.3f}\")\n",
    "if ps_rejected > ps_chosen:\n",
    "    print(\"  ⚠ WARNING: rejected group has higher problem_solution than chosen — negative correlation present.\")\n",
    "    print(\"  → See Notebook 04 for remedies (correctness floor, MOO, SFT warm-up).\")\n",
    "else:\n",
    "    print(\"  ✓ No negative correlation detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sample Efficiency Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funnel = pd.DataFrame([\n",
    "    {\"stage\": \"Raw data\",                  \"rows\": len(dataset)},\n",
    "    {\"stage\": \"After quality filter\",      \"rows\": len(df)},\n",
    "    {\"stage\": \"Weighted-sum pairs\",        \"rows\": len(df_weighted)},\n",
    "    {\"stage\": \"Single-perspective (style)\",\"rows\": len(df_style)},\n",
    "])\n",
    "funnel[\"retention_%\"] = (funnel[\"rows\"] / funnel[\"rows\"].iloc[0] * 100).round(1)\n",
    "print(funnel.to_string(index=False))\n",
    "print(\"\\nNote: aggressive multi-VJ filtering can reduce 24k → 700 pairs (see Notebook 04).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export DPO-Ready Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dpo_format(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convert to TRL DPO trainer expected schema.\"\"\"\n",
    "    return pd.DataFrame({\n",
    "        \"prompt\":   df[\"prompt_clean\"],\n",
    "        \"chosen\":   df[\"response_chosen\"],\n",
    "        \"rejected\": df[\"response_rejected\"],\n",
    "    })\n",
    "\n",
    "\n",
    "for name, subset in [(\"weighted\", df_weighted), (\"style\", df_style)]:\n",
    "    out = to_dpo_format(subset)\n",
    "    path = DATA_DIR / f\"dpo_{name}.jsonl\"\n",
    "    out.to_json(path, orient=\"records\", lines=True)\n",
    "    print(f\"Saved {len(out):,} pairs → {path}\")\n",
    "\n",
    "# Also save as HuggingFace Dataset\n",
    "ds_weighted = Dataset.from_pandas(to_dpo_format(df_weighted))\n",
    "ds_weighted.save_to_disk(str(DATA_DIR / \"dpo_weighted_hf\"))\n",
    "print(\"HuggingFace dataset saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Dataset | Pairs | Notes |\n",
    "|---------|-------|-------|\n",
    "| `dpo_weighted.jsonl` | ~N | Reward delta ≥ 0.05 across all VJs |\n",
    "| `dpo_style.jsonl`    | ~N | Style delta ≥ 0.10 + correctness floor ≥ 0.60 |\n",
    "\n",
    "**Next:** `02_finetuning_qlora.ipynb` — SFT warm-up → DPO on these pairs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
