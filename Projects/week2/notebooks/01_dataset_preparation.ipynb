{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 — Notebook 1: Dataset Preparation & DPO Pair Construction\n",
    "\n",
    "This notebook covers:\n",
    "1. Loading and inspecting raw data (~90k rows)\n",
    "2. De-identification with standard libraries (no LLMs)\n",
    "3. Data quality assessment\n",
    "4. Constructing chosen/rejected pairs for DPO via two strategies:\n",
    "   - **Weighted sum** scoring\n",
    "   - **Single-perspective filtering** (isolating one metric delta)\n",
    "5. Sample efficiency analysis — how much survives aggressive filtering\n",
    "\n",
    "---\n",
    "> **GPU requirement:** None — this is CPU/data-engineering work.  \n",
    "> **Estimated runtime:** ~10–20 min depending on dataset size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Environment & Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ysjdu2pjqo",
   "source": "## 0. Colab Setup (skip if running locally)\n\nRun this cell first when opening from Colab.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "g392jn7r2jn",
   "source": "import sys, os\n\nIN_COLAB = \"google.colab\" in sys.modules\nif IN_COLAB:\n    # Install dependencies\n    os.system(\"pip install -q datasets pandas pyarrow presidio-analyzer presidio-anonymizer spacy tqdm\")\n    os.system(\"python -m spacy download en_core_web_lg -q\")\n\n    # Clone repo so relative paths work\n    if not os.path.exists(\"/content/agentic-ai-learning\"):\n        os.system(\"git clone -q https://github.com/amnghd/agentic-ai-learning.git /content/agentic-ai-learning\")\n\n    # Set working directory to this notebook's folder\n    os.chdir(\"/content/agentic-ai-learning/Projects/week2/notebooks\")\n    os.makedirs(\"../data\", exist_ok=True)\n    print(\"Colab setup complete. Working dir:\", os.getcwd())\nelse:\n    print(\"Running locally — no setup needed.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting datasets\n",
      "  Using cached datasets-4.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: pandas in /Users/aminghd/Library/Python/3.9/lib/python/site-packages (2.3.3)\n",
      "Collecting pyarrow\n",
      "  Using cached pyarrow-21.0.0-cp39-cp39-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting presidio-analyzer\n",
      "  Using cached presidio_analyzer-2.2.360-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting presidio-anonymizer\n",
      "  Using cached presidio_anonymizer-2.2.360-py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting spacy\n",
      "  Using cached spacy-3.8.11.tar.gz (1.3 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Users/aminghd/Library/Python/3.9/lib/python/site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in /Users/aminghd/Library/Python/3.9/lib/python/site-packages (from datasets) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/aminghd/Library/Python/3.9/lib/python/site-packages (from datasets) (1.26.4)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/aminghd/Library/Python/3.9/lib/python/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /Users/aminghd/Library/Python/3.9/lib/python/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /Users/aminghd/Library/Python/3.9/lib/python/site-packages (from datasets) (3.6.0)\n",
      "Collecting multiprocess<0.70.19 (from datasets)\n",
      "  Downloading multiprocess-0.70.18-py39-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /Users/aminghd/Library/Python/3.9/lib/python/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /Users/aminghd/Library/Python/3.9/lib/python/site-packages (from datasets) (1.2.3)\n",
      "Requirement already satisfied: packaging in /Users/aminghd/Library/Python/3.9/lib/python/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/aminghd/Library/Python/3.9/lib/python/site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/aminghd/Library/Python/3.9/lib/python/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /Users/aminghd/Library/Python/3.9/lib/python/site-packages (from httpx<1.0.0->datasets) (4.12.0)\n",
      "Requirement already satisfied: certifi in /Users/aminghd/Library/Python/3.9/lib/python/site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/aminghd/Library/Python/3.9/lib/python/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/aminghd/Library/Python/3.9/lib/python/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/aminghd/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /Users/aminghd/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: shellingham in /Users/aminghd/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in /Users/aminghd/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/aminghd/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/aminghd/Library/Python/3.9/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/aminghd/Library/Python/3.9/lib/python/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/aminghd/Library/Python/3.9/lib/python/site-packages (from pandas) (2025.3)\n",
      "Collecting phonenumbers<10.0.0,>=8.12 (from presidio-analyzer)\n",
      "  Downloading phonenumbers-9.0.24-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: regex in /Users/aminghd/Library/Python/3.9/lib/python/site-packages (from presidio-analyzer) (2025.11.3)\n",
      "Collecting tldextract (from presidio-analyzer)\n",
      "  Downloading tldextract-5.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting cryptography<44.1 (from presidio-anonymizer)\n",
      "  Downloading cryptography-44.0.3-cp39-abi3-macosx_10_9_universal2.whl.metadata (5.7 kB)\n",
      "Collecting cffi>=1.12 (from cryptography<44.1->presidio-anonymizer)\n",
      "  Downloading cffi-2.0.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (2.6 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Using cached murmurhash-1.0.15-cp39-cp39-macosx_11_0_arm64.whl.metadata (2.3 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Using cached cymem-2.0.13-cp39-cp39-macosx_11_0_arm64.whl.metadata (9.7 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Using cached preshed-3.0.12-cp39-cp39-macosx_11_0_arm64.whl.metadata (2.5 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Using cached thinc-8.3.9-cp39-cp39-macosx_10_9_universal2.whl\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Using cached srsly-2.5.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.4.2 (from spacy)\n",
      "  Downloading weasel-0.4.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/aminghd/Library/Python/3.9/lib/python/site-packages (from spacy) (2.12.5)\n",
      "Collecting jinja2 (from spacy)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: setuptools in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from spacy) (58.0.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/aminghd/Library/Python/3.9/lib/python/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/aminghd/Library/Python/3.9/lib/python/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/aminghd/Library/Python/3.9/lib/python/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/aminghd/Library/Python/3.9/lib/python/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/aminghd/Library/Python/3.9/lib/python/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Using cached blis-1.3.3-cp39-cp39-macosx_10_9_universal2.whl\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/aminghd/Library/Python/3.9/lib/python/site-packages (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (8.1.8)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Downloading cloudpathlib-0.23.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Downloading smart_open-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Downloading wrapt-2.1.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/aminghd/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/aminghd/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/aminghd/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/aminghd/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/aminghd/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/aminghd/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/aminghd/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/aminghd/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Collecting pycparser (from cffi>=1.12->cryptography<44.1->presidio-anonymizer)\n",
      "  Downloading pycparser-2.23-py3-none-any.whl.metadata (993 bytes)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/aminghd/Library/Python/3.9/lib/python/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->spacy)\n",
      "  Downloading markupsafe-3.0.3-cp39-cp39-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
      "Collecting requests-file>=1.4 (from tldextract->presidio-analyzer)\n",
      "  Downloading requests_file-3.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Downloading datasets-4.5.0-py3-none-any.whl (515 kB)\n",
      "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading multiprocess-0.70.18-py39-none-any.whl (133 kB)\n",
      "Downloading pyarrow-21.0.0-cp39-cp39-macosx_12_0_arm64.whl (31.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.2/31.2 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading presidio_analyzer-2.2.360-py3-none-any.whl (128 kB)\n",
      "Downloading phonenumbers-9.0.24-py2.py3-none-any.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading presidio_anonymizer-2.2.360-py3-none-any.whl (35 kB)\n",
      "Downloading cryptography-44.0.3-cp39-abi3-macosx_10_9_universal2.whl (6.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Using cached cymem-2.0.13-cp39-cp39-macosx_11_0_arm64.whl (43 kB)\n",
      "Using cached murmurhash-1.0.15-cp39-cp39-macosx_11_0_arm64.whl (27 kB)\n",
      "Using cached preshed-3.0.12-cp39-cp39-macosx_11_0_arm64.whl (126 kB)\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Using cached srsly-2.5.2-cp39-cp39-macosx_11_0_arm64.whl (654 kB)\n",
      "Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.3-py3-none-any.whl (50 kB)\n",
      "Downloading cloudpathlib-0.23.0-py3-none-any.whl (62 kB)\n",
      "Downloading smart_open-7.5.0-py3-none-any.whl (63 kB)\n",
      "Downloading cffi-2.0.0-cp39-cp39-macosx_11_0_arm64.whl (180 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading markupsafe-3.0.3-cp39-cp39-macosx_11_0_arm64.whl (12 kB)\n",
      "Downloading pycparser-2.23-py3-none-any.whl (118 kB)\n",
      "Downloading tldextract-5.3.0-py3-none-any.whl (107 kB)\n",
      "Downloading requests_file-3.0.1-py2.py3-none-any.whl (4.5 kB)\n",
      "Downloading wrapt-2.1.1-cp39-cp39-macosx_11_0_arm64.whl (61 kB)\n",
      "Building wheels for collected packages: spacy\n",
      "  Building wheel for spacy (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for spacy: filename=spacy-3.8.11-cp39-cp39-macosx_10_9_universal2.whl size=11516831 sha256=8a3a81c9ed697eab8784286821972dafcc86e4839c32b76567a568da5a309f28\n",
      "  Stored in directory: /Users/aminghd/Library/Caches/pip/wheels/a8/ad/31/4477e5bf199f83e76944b01dce246860fa577d6ee5126300e8\n",
      "Successfully built spacy\n",
      "Installing collected packages: wrapt, wasabi, spacy-loggers, spacy-legacy, pycparser, pyarrow, phonenumbers, murmurhash, MarkupSafe, dill, cymem, cloudpathlib, catalogue, blis, srsly, smart-open, requests-file, preshed, multiprocess, jinja2, cffi, tldextract, cryptography, confection, weasel, thinc, presidio-anonymizer, spacy, datasets, presidio-analyzer\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30/30\u001b[0m [presidio-analyzer]datasets]ess]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.3 blis-1.3.3 catalogue-2.0.10 cffi-2.0.0 cloudpathlib-0.23.0 confection-0.1.5 cryptography-44.0.3 cymem-2.0.13 datasets-4.5.0 dill-0.4.0 jinja2-3.1.6 multiprocess-0.70.18 murmurhash-1.0.15 phonenumbers-9.0.24 preshed-3.0.12 presidio-analyzer-2.2.360 presidio-anonymizer-2.2.360 pyarrow-21.0.0 pycparser-2.23 requests-file-3.0.1 smart-open-7.5.0 spacy-3.8.11 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.2 thinc-8.3.9 tldextract-5.3.0 wasabi-1.1.3 weasel-0.4.3 wrapt-2.1.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "zsh:1: command not found: python\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies if needed (comment out after first run)\n",
    "# \n",
    "# !pip3 install datasets pandas pyarrow presidio-analyzer presidio-anonymizer spacy tqdm\n",
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aminghd/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/aminghd/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports OK\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path(\"../data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Imports OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Raw Dataset\n",
    "\n",
    "We use the **Argilla Distilabel Customer Support** dataset as a realistic proxy.  \n",
    "Replace `DATASET_PATH` with your own Spark-processed parquet/JSONL if you have it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train_prefs split: 100%|██████████| 61135/61135 [00:00<00:00, 258697.12 examples/s]\n",
      "Generating train_sft split: 100%|██████████| 61135/61135 [00:00<00:00, 453055.92 examples/s]\n",
      "Generating test_prefs split: 100%|██████████| 2000/2000 [00:00<00:00, 213304.04 examples/s]\n",
      "Generating test_sft split: 100%|██████████| 1000/1000 [00:00<00:00, 162098.71 examples/s]\n",
      "Generating train_gen split: 100%|██████████| 61135/61135 [00:00<00:00, 390110.38 examples/s]\n",
      "Generating test_gen split: 100%|██████████| 1000/1000 [00:00<00:00, 191197.70 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (61135, 7)\n",
      "Columns: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>chosen</th>\n",
       "      <th>rejected</th>\n",
       "      <th>messages</th>\n",
       "      <th>score_chosen</th>\n",
       "      <th>score_rejected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how can i develop a habit of drawing daily</td>\n",
       "      <td>086b3e24f29b8956a01059f79c56db35d118a06fb6b844...</td>\n",
       "      <td>[{'content': 'how can i develop a habit of dra...</td>\n",
       "      <td>[{'content': 'how can i develop a habit of dra...</td>\n",
       "      <td>[{'content': 'how can i develop a habit of dra...</td>\n",
       "      <td>8.5</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how can I transform the getPosition method of ...</td>\n",
       "      <td>2766cbd1fed7f982d94b031596e771c841668bd8913839...</td>\n",
       "      <td>[{'content': 'how can I transform the getPosit...</td>\n",
       "      <td>[{'content': 'how can I transform the getPosit...</td>\n",
       "      <td>[{'content': 'how can I transform the getPosit...</td>\n",
       "      <td>6.5</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Given a sentence in French, provide an equival...</td>\n",
       "      <td>0efb42706b3fcc906f579505c7cc0c4e68a640ab3862b1...</td>\n",
       "      <td>[{'content': 'Given a sentence in French, prov...</td>\n",
       "      <td>[{'content': 'Given a sentence in French, prov...</td>\n",
       "      <td>[{'content': 'Given a sentence in French, prov...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0         how can i develop a habit of drawing daily   \n",
       "1  how can I transform the getPosition method of ...   \n",
       "2  Given a sentence in French, provide an equival...   \n",
       "\n",
       "                                           prompt_id  \\\n",
       "0  086b3e24f29b8956a01059f79c56db35d118a06fb6b844...   \n",
       "1  2766cbd1fed7f982d94b031596e771c841668bd8913839...   \n",
       "2  0efb42706b3fcc906f579505c7cc0c4e68a640ab3862b1...   \n",
       "\n",
       "                                              chosen  \\\n",
       "0  [{'content': 'how can i develop a habit of dra...   \n",
       "1  [{'content': 'how can I transform the getPosit...   \n",
       "2  [{'content': 'Given a sentence in French, prov...   \n",
       "\n",
       "                                            rejected  \\\n",
       "0  [{'content': 'how can i develop a habit of dra...   \n",
       "1  [{'content': 'how can I transform the getPosit...   \n",
       "2  [{'content': 'Given a sentence in French, prov...   \n",
       "\n",
       "                                            messages  score_chosen  \\\n",
       "0  [{'content': 'how can i develop a habit of dra...           8.5   \n",
       "1  [{'content': 'how can I transform the getPosit...           6.5   \n",
       "2  [{'content': 'Given a sentence in French, prov...           6.0   \n",
       "\n",
       "   score_rejected  \n",
       "0             8.5  \n",
       "1             6.5  \n",
       "2             3.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Option A: Load from HuggingFace Hub (public proxy dataset) ---\n",
    "dataset = load_dataset(\"HuggingFaceH4/ultrafeedback_binarized\", split=\"train_prefs\")\n",
    "df = dataset.to_pandas()\n",
    "\n",
    "# --- Option B: Load your own data ---\n",
    "# df = pd.read_parquet(DATA_DIR / \"rollouts_raw.parquet\")\n",
    "# df = pd.read_json(DATA_DIR / \"rollouts_raw.jsonl\", lines=True)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt             object\n",
      "prompt_id          object\n",
      "chosen             object\n",
      "rejected           object\n",
      "messages           object\n",
      "score_chosen      float64\n",
      "score_rejected    float64\n",
      "dtype: object\n",
      "\n",
      "Null counts:\n",
      "prompt            0\n",
      "prompt_id         0\n",
      "chosen            0\n",
      "rejected          0\n",
      "messages          0\n",
      "score_chosen      0\n",
      "score_rejected    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Quick stats\n",
    "print(df.dtypes)\n",
    "print(\"\\nNull counts:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. De-identification (Standard Libraries)\n",
    "\n",
    "Using **Microsoft Presidio** — no LLM calls, fully deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presidio loaded.\n",
      "Before: Hi, I'm John Smith and my email is john@example.com, phone 555-1234.\n",
      "After:  Hi, I'm <PERSON> and my email is <EMAIL_ADDRESS>, phone <DATE_TIME>.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from presidio_analyzer import AnalyzerEngine\n",
    "    from presidio_anonymizer import AnonymizerEngine\n",
    "\n",
    "    analyzer = AnalyzerEngine()\n",
    "    anonymizer = AnonymizerEngine()\n",
    "    PRESIDIO_AVAILABLE = True\n",
    "    print(\"Presidio loaded.\")\n",
    "except ImportError:\n",
    "    PRESIDIO_AVAILABLE = False\n",
    "    print(\"Presidio not installed — skipping de-id (install presidio-analyzer presidio-anonymizer spacy + en_core_web_lg)\")\n",
    "\n",
    "\n",
    "def deidentify(text: str) -> str:\n",
    "    \"\"\"Replace PII entities with type placeholders, e.g. <PERSON>.\"\"\"\n",
    "    if not PRESIDIO_AVAILABLE or not isinstance(text, str):\n",
    "        return text\n",
    "    results = analyzer.analyze(text=text, language=\"en\")\n",
    "    anonymized = anonymizer.anonymize(text=text, analyzer_results=results)\n",
    "    return anonymized.text\n",
    "\n",
    "\n",
    "# Test\n",
    "sample = \"Hi, I'm John Smith and my email is john@example.com, phone 555-1234.\"\n",
    "print(\"Before:\", sample)\n",
    "print(\"After: \", deidentify(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "De-identifying prompts: 100%|██████████| 61135/61135 [23:06<00:00, 44.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records modified by de-id: 31156/61135 (51.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply to the prompt column (adjust column name to your schema)\n",
    "PROMPT_COL = \"prompt\"  # change if your column is named differently\n",
    "\n",
    "if PROMPT_COL in df.columns and PRESIDIO_AVAILABLE:\n",
    "    tqdm.pandas(desc=\"De-identifying prompts\")\n",
    "    df[\"prompt_clean\"] = df[PROMPT_COL].progress_apply(deidentify)\n",
    "    changed = (df[\"prompt_clean\"] != df[PROMPT_COL]).sum()\n",
    "    print(f\"Records modified by de-id: {changed}/{len(df)} ({changed/len(df)*100:.1f}%)\")\n",
    "else:\n",
    "    df[\"prompt_clean\"] = df.get(PROMPT_COL, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Quality Summary ===\n",
      "Total rows        : 61,135\n",
      "Empty prompts     : 0\n",
      "Duplicate prompts : 89\n",
      "Contains URL      : 38\n",
      "\n",
      "Prompt length (words):\n",
      "count    61135.0\n",
      "mean       106.0\n",
      "std        146.8\n",
      "min          1.0\n",
      "25%         18.0\n",
      "50%         57.0\n",
      "75%        122.0\n",
      "max       2312.0\n"
     ]
    }
   ],
   "source": [
    "def quality_metrics(df: pd.DataFrame, text_col: str) -> pd.DataFrame:\n",
    "    \"\"\"Compute basic quality signals on a text column.\"\"\"\n",
    "    d = df.copy()\n",
    "    d[\"len_chars\"] = d[text_col].str.len()\n",
    "    d[\"len_words\"] = d[text_col].str.split().str.len()\n",
    "    d[\"is_empty\"] = d[text_col].str.strip().str.len() == 0\n",
    "    d[\"has_url\"] = d[text_col].str.contains(r\"https?://\", na=False)\n",
    "    d[\"is_duplicate\"] = d.duplicated(subset=[text_col], keep=False)\n",
    "    return d\n",
    "\n",
    "\n",
    "df = quality_metrics(df, \"prompt_clean\")\n",
    "\n",
    "print(\"=== Quality Summary ===\")\n",
    "print(f\"Total rows        : {len(df):,}\")\n",
    "print(f\"Empty prompts     : {df['is_empty'].sum():,}\")\n",
    "print(f\"Duplicate prompts : {df['is_duplicate'].sum():,}\")\n",
    "print(f\"Contains URL      : {df['has_url'].sum():,}\")\n",
    "print(f\"\\nPrompt length (words):\")\n",
    "print(df[\"len_words\"].describe().round(1).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after quality filter: 61,007 (removed 128)\n"
     ]
    }
   ],
   "source": [
    "# Filter out bad rows\n",
    "before = len(df)\n",
    "df = df[\n",
    "    ~df[\"is_empty\"]\n",
    "    & ~df.duplicated(subset=[\"prompt_clean\"], keep=\"first\")\n",
    "    & df[\"len_words\"].between(5, 2000)\n",
    "].reset_index(drop=True)\n",
    "print(f\"Rows after quality filter: {len(df):,} (removed {before - len(df):,})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simulating VJ (Virtual Judge) Scores\n",
    "\n",
    "In production these come from your scoring pipeline.  \n",
    "Here we attach synthetic scores to demonstrate the pair-construction logic — swap in real columns when available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       score_chosen  score_rejected  score_correctness_chosen  \\\n",
      "count     61007.000       61007.000                 61007.000   \n",
      "mean          7.825           5.954                     0.749   \n",
      "std           1.127           1.986                     0.118   \n",
      "min           1.000           1.000                     0.223   \n",
      "25%           7.500           4.000                     0.669   \n",
      "50%           8.000           6.500                     0.749   \n",
      "75%           8.500           7.500                     0.830   \n",
      "max          10.000          10.000                     1.000   \n",
      "\n",
      "       score_correctness_rejected  score_groundedness_chosen  \\\n",
      "count                   61007.000                  61007.000   \n",
      "mean                        0.499                      0.720   \n",
      "std                         0.151                      0.119   \n",
      "min                         0.000                      0.142   \n",
      "25%                         0.397                      0.640   \n",
      "50%                         0.498                      0.721   \n",
      "75%                         0.601                      0.802   \n",
      "max                         1.000                      1.000   \n",
      "\n",
      "       score_groundedness_rejected  score_problem_solution_chosen  \\\n",
      "count                    61007.000                      61007.000   \n",
      "mean                         0.550                          0.700   \n",
      "std                          0.149                          0.119   \n",
      "min                          0.000                          0.227   \n",
      "25%                          0.450                          0.619   \n",
      "50%                          0.551                          0.700   \n",
      "75%                          0.651                          0.781   \n",
      "max                          1.000                          1.000   \n",
      "\n",
      "       score_problem_solution_rejected  score_style_chosen  \\\n",
      "count                         61007.00           61007.000   \n",
      "mean                              0.52               0.650   \n",
      "std                               0.15               0.120   \n",
      "min                               0.00               0.142   \n",
      "25%                               0.42               0.569   \n",
      "50%                               0.52               0.650   \n",
      "75%                               0.62               0.731   \n",
      "max                               1.00               1.000   \n",
      "\n",
      "       score_style_rejected  \n",
      "count             61007.000  \n",
      "mean                  0.480  \n",
      "std                   0.150  \n",
      "min                   0.000  \n",
      "25%                   0.379  \n",
      "50%                   0.479  \n",
      "75%                   0.581  \n",
      "max                   1.000  \n"
     ]
    }
   ],
   "source": [
    "# --- If your dataframe already has score columns, skip this cell ---\n",
    "# Expected schema after this cell:\n",
    "#   score_correctness    : float [0,1]\n",
    "#   score_groundedness   : float [0,1]\n",
    "#   score_problem_solution: float [0,1]\n",
    "#   score_style          : float [0,1]\n",
    "#   response_chosen      : str\n",
    "#   response_rejected    : str\n",
    "\n",
    "def _extract_text(cell):\n",
    "    \"\"\"Handle UltraFeedback schema where responses are lists of dicts.\"\"\"\n",
    "    if isinstance(cell, list) and len(cell) > 0:\n",
    "        item = cell[0]\n",
    "        if isinstance(item, dict):\n",
    "            return item.get(\"content\", str(item))\n",
    "        return str(item)\n",
    "    return str(cell)\n",
    "\n",
    "\n",
    "# Extract chosen / rejected text\n",
    "if \"chosen\" in df.columns:\n",
    "    df[\"response_chosen\"]   = df[\"chosen\"].apply(_extract_text)\n",
    "    df[\"response_rejected\"] = df[\"rejected\"].apply(_extract_text)\n",
    "else:\n",
    "    df[\"response_chosen\"]   = \"placeholder chosen response\"\n",
    "    df[\"response_rejected\"] = \"placeholder rejected response\"\n",
    "\n",
    "# Simulate per-response VJ scores (replace with real scores in production)\n",
    "rng = np.random.default_rng(42)\n",
    "n = len(df)\n",
    "for col, mu_chosen, mu_rejected in [\n",
    "    (\"score_correctness\",     0.75, 0.50),\n",
    "    (\"score_groundedness\",    0.72, 0.55),\n",
    "    (\"score_problem_solution\",0.70, 0.52),\n",
    "    (\"score_style\",           0.65, 0.48),\n",
    "]:\n",
    "    df[f\"{col}_chosen\"]   = rng.normal(mu_chosen,   0.12, n).clip(0, 1)\n",
    "    df[f\"{col}_rejected\"] = rng.normal(mu_rejected, 0.15, n).clip(0, 1)\n",
    "\n",
    "print(df[[c for c in df.columns if c.startswith(\"score_\")]].describe().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Strategy A — Weighted Sum Pair Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs surviving weighted-sum filter (delta >= 0.05):\n",
      "  56,805 / 61,007 (93.1%)\n",
      "count    56805.000\n",
      "mean         0.219\n",
      "std          0.091\n",
      "min          0.050\n",
      "25%          0.151\n",
      "50%          0.214\n",
      "75%          0.280\n",
      "max          0.641\n",
      "Name: reward_delta, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "WEIGHTS = {\n",
    "    \"score_correctness\":      0.40,\n",
    "    \"score_groundedness\":     0.25,\n",
    "    \"score_problem_solution\": 0.25,\n",
    "    \"score_style\":            0.10,\n",
    "}\n",
    "\n",
    "\n",
    "def weighted_reward(df: pd.DataFrame, suffix: str, weights: dict) -> pd.Series:\n",
    "    return sum(w * df[f\"{k}_{suffix}\"] for k, w in weights.items())\n",
    "\n",
    "\n",
    "df[\"reward_chosen\"]   = weighted_reward(df, \"chosen\",   WEIGHTS)\n",
    "df[\"reward_rejected\"] = weighted_reward(df, \"rejected\", WEIGHTS)\n",
    "df[\"reward_delta\"]    = df[\"reward_chosen\"] - df[\"reward_rejected\"]\n",
    "\n",
    "# Ensure chosen > rejected (filter out noise)\n",
    "DELTA_THRESHOLD = 0.05\n",
    "df_weighted = df[df[\"reward_delta\"] >= DELTA_THRESHOLD].copy()\n",
    "\n",
    "print(f\"Pairs surviving weighted-sum filter (delta >= {DELTA_THRESHOLD}):\")\n",
    "print(f\"  {len(df_weighted):,} / {len(df):,} ({len(df_weighted)/len(df)*100:.1f}%)\")\n",
    "print(df_weighted[\"reward_delta\"].describe().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Strategy B — Single-Perspective Filtering (Isolating Style Delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs surviving single-perspective style filter:\n",
      "  1,006 / 61,007 (1.6%)\n",
      "\n",
      "[Sanity] problem_solution — chosen avg: 0.691, rejected avg: 0.509\n",
      "  ✓ No negative correlation detected.\n"
     ]
    }
   ],
   "source": [
    "STYLE_DELTA_MIN    = 0.50   # chosen must score >= this better on style\n",
    "CORRECTNESS_FLOOR  = 0.78   # chosen must maintain at least this correctness\n",
    "\n",
    "df[\"style_delta\"] = (\n",
    "    df[\"score_style_chosen\"] - df[\"score_style_rejected\"]\n",
    ")\n",
    "\n",
    "df_style = df[\n",
    "    (df[\"style_delta\"] >= STYLE_DELTA_MIN)\n",
    "    & (df[\"score_correctness_chosen\"] >= CORRECTNESS_FLOOR)  # correctness floor!\n",
    "].copy()\n",
    "\n",
    "print(f\"Pairs surviving single-perspective style filter:\")\n",
    "print(f\"  {len(df_style):,} / {len(df):,} ({len(df_style)/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Sanity check: is the rejected group's problem_solution perversely higher?\n",
    "ps_chosen   = df_style[\"score_problem_solution_chosen\"].mean()\n",
    "ps_rejected = df_style[\"score_problem_solution_rejected\"].mean()\n",
    "print(f\"\\n[Sanity] problem_solution — chosen avg: {ps_chosen:.3f}, rejected avg: {ps_rejected:.3f}\")\n",
    "if ps_rejected > ps_chosen:\n",
    "    print(\"  ⚠ WARNING: rejected group has higher problem_solution than chosen — negative correlation present.\")\n",
    "    print(\"  → See Notebook 04 for remedies (correctness floor, MOO, SFT warm-up).\")\n",
    "else:\n",
    "    print(\"  ✓ No negative correlation detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sample Efficiency Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     stage  rows  retention_%\n",
      "                  Raw data 61135        100.0\n",
      "      After quality filter 61007         99.8\n",
      "        Weighted-sum pairs 56805         92.9\n",
      "Single-perspective (style)  1006          1.6\n",
      "\n",
      "Note: aggressive multi-VJ filtering can reduce 24k → 700 pairs (see Notebook 04).\n"
     ]
    }
   ],
   "source": [
    "funnel = pd.DataFrame([\n",
    "    {\"stage\": \"Raw data\",                  \"rows\": len(dataset)},\n",
    "    {\"stage\": \"After quality filter\",      \"rows\": len(df)},\n",
    "    {\"stage\": \"Weighted-sum pairs\",        \"rows\": len(df_weighted)},\n",
    "    {\"stage\": \"Single-perspective (style)\",\"rows\": len(df_style)},\n",
    "])\n",
    "funnel[\"retention_%\"] = (funnel[\"rows\"] / funnel[\"rows\"].iloc[0] * 100).round(1)\n",
    "print(funnel.to_string(index=False))\n",
    "print(\"\\nNote: aggressive multi-VJ filtering can reduce 24k → 700 pairs (see Notebook 04).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export DPO-Ready Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 56,805 pairs → ../data/dpo_weighted.jsonl\n",
      "Saved 1,006 pairs → ../data/dpo_style.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 56805/56805 [00:00<00:00, 900266.91 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuggingFace dataset saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def to_dpo_format(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convert to TRL DPO trainer expected schema.\"\"\"\n",
    "    return pd.DataFrame({\n",
    "        \"prompt\":   df[\"prompt_clean\"],\n",
    "        \"chosen\":   df[\"response_chosen\"],\n",
    "        \"rejected\": df[\"response_rejected\"],\n",
    "    })\n",
    "\n",
    "\n",
    "for name, subset in [(\"weighted\", df_weighted), (\"style\", df_style)]:\n",
    "    out = to_dpo_format(subset)\n",
    "    path = DATA_DIR / f\"dpo_{name}.jsonl\"\n",
    "    out.to_json(path, orient=\"records\", lines=True)\n",
    "    print(f\"Saved {len(out):,} pairs → {path}\")\n",
    "\n",
    "# Also save as HuggingFace Dataset\n",
    "ds_weighted = Dataset.from_pandas(to_dpo_format(df_weighted))\n",
    "ds_weighted.save_to_disk(str(DATA_DIR / \"dpo_weighted_hf\"))\n",
    "print(\"HuggingFace dataset saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Dataset | Pairs | Notes |\n",
    "|---------|-------|-------|\n",
    "| `dpo_weighted.jsonl` | ~N | Reward delta ≥ 0.05 across all VJs |\n",
    "| `dpo_style.jsonl`    | ~N | Style delta ≥ 0.10 + correctness floor ≥ 0.60 |\n",
    "\n",
    "**Next:** `02_finetuning_qlora.ipynb` — SFT warm-up → DPO on these pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad166cfb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aab2c27b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2dca4991",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "133122dc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e91a4b21",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
