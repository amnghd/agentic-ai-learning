{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Week 2 — Notebook 3: Model Evaluation & VJ Scoring\n\nMeasures the impact of SFT + DPO across four dimensions:  \n`correctness` · `groundedness` · `problem_solution` · `style`\n\n**Workflow (two steps):**\n1. **Generation** — run `eval_modal.py` from terminal to generate responses on A100 and save `responses.jsonl` to the Modal volume, then download it.\n2. **Scoring & analysis** — run this notebook locally (CPU is fine) to score responses via OpenAI API and visualise results.\n\nCovers:\n1. Loading pre-generated responses from `eval_modal.py`\n2. VJ (Virtual Judge) scoring via GPT-4o-mini\n3. **Meta-eval** — Judge-on-Judge agreement check\n4. Before/after comparison: alignment tax detection\n5. Score distribution & correlation analysis\n\n---\n> **GPU:** not needed here — generation is handled by `eval_modal.py`  \n> **Cost estimate:** ~$3–8 for 500 scored samples at GPT-4o-mini rates"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai transformers peft bitsandbytes datasets pandas seaborn scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport json\nimport time\nfrom pathlib import Path\nfrom typing import Optional\nfrom dotenv import load_dotenv\n\nload_dotenv(\"../../week1/.env\")\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom openai import OpenAI\nfrom tqdm.auto import tqdm\n\nDATA_DIR  = Path(\"../data\")\nEVAL_DIR  = DATA_DIR / \"eval_results\"\nEVAL_DIR.mkdir(parents=True, exist_ok=True)\n\n# responses.jsonl is generated by eval_modal.py and downloaded from the Modal volume\nRESPONSES_FILE = EVAL_DIR / \"responses.jsonl\"\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nJUDGE_MODEL = \"gpt-4o-mini\"   # upgrade to gpt-4o for higher precision meta-eval\n\nprint(\"Imports OK\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. VJ Scoring Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORE_PROMPT = \"\"\"\\\n",
    "You are an expert evaluator for a customer support AI system.\n",
    "Score the RESPONSE to the USER QUERY on these four dimensions.\n",
    "Return ONLY valid JSON with keys: correctness, groundedness, problem_solution, style.\n",
    "Each value must be a float between 0.0 and 1.0.\n",
    "\n",
    "Definitions:\n",
    "- correctness: factual accuracy, no hallucinations\n",
    "- groundedness: claims are supported by plausible context, no fabrications\n",
    "- problem_solution: response actually solves or addresses the user's problem\n",
    "- style: tone is professional, empathetic, concise\n",
    "\n",
    "USER QUERY: {prompt}\n",
    "RESPONSE: {response}\n",
    "\n",
    "JSON:\"\"\"\n",
    "\n",
    "\n",
    "def score_response(\n",
    "    prompt: str,\n",
    "    response: str,\n",
    "    model: str = JUDGE_MODEL,\n",
    "    retries: int = 3,\n",
    ") -> Optional[dict]:\n",
    "    \"\"\"Score a single response across 4 VJ dimensions.\"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": SCORE_PROMPT.format(prompt=prompt[:600], response=response[:600]),\n",
    "                }],\n",
    "                temperature=0.0,\n",
    "                max_tokens=100,\n",
    "            )\n",
    "            return json.loads(completion.choices[0].message.content.strip())\n",
    "        except Exception as e:\n",
    "            if attempt == retries - 1:\n",
    "                print(f\"Scoring failed: {e}\")\n",
    "                return None\n",
    "            time.sleep(2 ** attempt)\n",
    "\n",
    "\n",
    "# Test\n",
    "test = score_response(\n",
    "    \"My order hasn't arrived after 2 weeks.\",\n",
    "    \"I apologize for the delay. Please contact support@shop.com with your order number.\"\n",
    ")\n",
    "print(\"Test score:\", test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Load Pre-Generated Responses\n\nResponses are generated on A100 via `eval_modal.py`.\n\n**To generate (run once from terminal):**\n```bash\ncd Projects/week2\nmodal run eval_modal.py\n# then download:\nmodal volume get week2-models /responses.jsonl ./data/eval_results/responses.jsonl\n```\n\nThen come back here and run this cell."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load responses generated by eval_modal.py\nif not RESPONSES_FILE.exists():\n    raise FileNotFoundError(\n        f\"{RESPONSES_FILE} not found.\\n\"\n        \"Run eval_modal.py first:\\n\"\n        \"  modal run eval_modal.py\\n\"\n        \"  modal volume get week2-models /responses.jsonl ./data/eval_results/responses.jsonl\"\n    )\n\ndf_responses = pd.read_json(RESPONSES_FILE, lines=True)\nprint(f\"Loaded {len(df_responses):,} rows\")\nprint(f\"Checkpoints: {df_responses['checkpoint'].unique().tolist()}\")\nprint(df_responses.head(2))\n\n# Extract for scoring loop\nprompts = df_responses[df_responses[\"checkpoint\"] == \"base\"][\"prompt\"].tolist()\nall_responses = {\n    ckpt: df_responses[df_responses[\"checkpoint\"] == ckpt][\"response\"].tolist()\n    for ckpt in df_responses[\"checkpoint\"].unique()\n}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Score All Responses with VJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIMS = [\"correctness\", \"groundedness\", \"problem_solution\", \"style\"]\n",
    "results = []\n",
    "\n",
    "for ckpt_name, responses in all_responses.items():\n",
    "    print(f\"Scoring: {ckpt_name}...\")\n",
    "    for prompt, response in tqdm(zip(prompts, responses), total=len(prompts)):\n",
    "        scores = score_response(prompt, response)\n",
    "        if scores:\n",
    "            results.append({\"checkpoint\": ckpt_name, \"prompt\": prompt, **scores})\n",
    "\n",
    "df_scores = pd.DataFrame(results)\n",
    "df_scores.to_json(EVAL_DIR / \"vj_scores.jsonl\", orient=\"records\", lines=True)\n",
    "print(f\"Scored {len(df_scores)} rows.\")\n",
    "df_scores.groupby(\"checkpoint\")[DIMS].mean().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Alignment Tax Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_scores = df_scores.groupby(\"checkpoint\")[DIMS].mean()\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "for ax, dim in zip(axes, DIMS):\n",
    "    vals = mean_scores[dim]\n",
    "    colors = [\"#4C72B0\" if v == vals.max() else \"#DD8452\" for v in vals]\n",
    "    ax.bar(vals.index, vals.values, color=colors)\n",
    "    ax.set_title(dim, fontsize=13)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axhline(vals[\"base\"], linestyle=\"--\", color=\"gray\", linewidth=1, label=\"base\")\n",
    "    for i, v in enumerate(vals.values):\n",
    "        ax.text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\", fontsize=10)\n",
    "\n",
    "plt.suptitle(\"VJ Scores: Base → SFT → DPO\", fontsize=15, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(EVAL_DIR / \"alignment_tax.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Flag alignment tax\n",
    "for dim in [\"correctness\", \"problem_solution\"]:\n",
    "    delta = mean_scores.loc[\"dpo\", dim] - mean_scores.loc[\"base\", dim]\n",
    "    if delta < -0.02:\n",
    "        print(f\"⚠  Alignment tax on '{dim}': {delta:+.3f}\")\n",
    "    else:\n",
    "        print(f\"✓  No significant tax on '{dim}': {delta:+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Meta-Eval — Judge-on-Judge Agreement\n",
    "\n",
    "Estimates VJ precision by comparing GPT-4o-mini scores to GPT-4o scores on 500 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "META_EVAL_N = 100   # raise to 500 for production; 100 here to control cost\n",
    "\n",
    "df_meta = df_scores[df_scores[\"checkpoint\"] == \"dpo\"].sample(META_EVAL_N, random_state=7)\n",
    "strong_judge_scores = []\n",
    "\n",
    "print(f\"Running meta-eval on {META_EVAL_N} samples with gpt-4o...\")\n",
    "for _, row in tqdm(df_meta.iterrows(), total=META_EVAL_N):\n",
    "    # Find corresponding response\n",
    "    idx = df_scores[\n",
    "        (df_scores[\"checkpoint\"] == \"dpo\") & (df_scores[\"prompt\"] == row[\"prompt\"])\n",
    "    ].index[0]\n",
    "    response = all_responses[\"dpo\"][prompts.index(row[\"prompt\"])] if row[\"prompt\"] in prompts else \"\"\n",
    "    s = score_response(row[\"prompt\"], response, model=\"gpt-4o\")\n",
    "    if s:\n",
    "        strong_judge_scores.append({\"prompt\": row[\"prompt\"], **{f\"strong_{k}\": v for k, v in s.items()}})\n",
    "\n",
    "df_meta_eval = df_meta.merge(\n",
    "    pd.DataFrame(strong_judge_scores), on=\"prompt\", how=\"inner\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== Judge Agreement (Pearson r): VJ-mini vs GPT-4o ===\")\n",
    "for dim in DIMS:\n",
    "    r, p = stats.pearsonr(df_meta_eval[dim], df_meta_eval[f\"strong_{dim}\"])\n",
    "    quality = \"good\" if r > 0.7 else \"moderate\" if r > 0.5 else \"⚠ LOW\"\n",
    "    print(f\"  {dim:<22} r={r:.3f}  p={p:.3f}  [{quality}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Score Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "for ax, ckpt in zip(axes, [\"base\", \"sft\", \"dpo\"]):\n",
    "    sub = df_scores[df_scores[\"checkpoint\"] == ckpt][DIMS]\n",
    "    sns.heatmap(\n",
    "        sub.corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\",\n",
    "        vmin=-1, vmax=1, ax=ax, cbar=False,\n",
    "    )\n",
    "    ax.set_title(f\"{ckpt} — score correlations\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(EVAL_DIR / \"score_correlations.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Negative style↔correctness correlation in DPO? That's the alignment tax in numbers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Checkpoint | correctness | problem_solution | style | alignment tax? |\n",
    "|------------|-------------|------------------|-------|----------------|\n",
    "| base       | —           | —                | —     | —              |\n",
    "| sft        | ↑           | ↑                | ~     | None           |\n",
    "| dpo        | ±           | ±                | ↑     | Check output   |\n",
    "\n",
    "**Next:** `04_alignment_pain_points.ipynb` — deeper analysis + remedies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
