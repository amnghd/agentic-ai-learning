{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 — Notebook 3: Model Evaluation & VJ Scoring\n",
    "\n",
    "Measures the impact of SFT + DPO across four dimensions:  \n",
    "`correctness` · `groundedness` · `problem_solution` · `style`\n",
    "\n",
    "Covers:\n",
    "1. Generating responses from base vs. SFT vs. DPO checkpoints\n",
    "2. VJ (Virtual Judge) scoring via GPT-4o\n",
    "3. **Meta-eval** — Judge-on-Judge agreement check (500-sample human review proxy)\n",
    "4. Before/after comparison: alignment tax detection\n",
    "5. Score distribution & correlation analysis\n",
    "\n",
    "---\n",
    "> **GPU requirement:** 1 GPU for inference; scoring calls OpenAI API.  \n",
    "> **Cost estimate:** ~\\$3–8 for 500 scored samples at GPT-4o-mini rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai transformers peft bitsandbytes datasets pandas seaborn scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../../week1/.env\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from openai import OpenAI\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "DATA_DIR   = Path(\"../data\")\n",
    "MODELS_DIR = Path(\"../models\")\n",
    "EVAL_DIR   = DATA_DIR / \"eval_results\"\n",
    "EVAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "JUDGE_MODEL = \"gpt-4o-mini\"   # upgrade to gpt-4o for higher precision meta-eval\n",
    "\n",
    "print(\"Imports OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. VJ Scoring Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORE_PROMPT = \"\"\"\\\n",
    "You are an expert evaluator for a customer support AI system.\n",
    "Score the RESPONSE to the USER QUERY on these four dimensions.\n",
    "Return ONLY valid JSON with keys: correctness, groundedness, problem_solution, style.\n",
    "Each value must be a float between 0.0 and 1.0.\n",
    "\n",
    "Definitions:\n",
    "- correctness: factual accuracy, no hallucinations\n",
    "- groundedness: claims are supported by plausible context, no fabrications\n",
    "- problem_solution: response actually solves or addresses the user's problem\n",
    "- style: tone is professional, empathetic, concise\n",
    "\n",
    "USER QUERY: {prompt}\n",
    "RESPONSE: {response}\n",
    "\n",
    "JSON:\"\"\"\n",
    "\n",
    "\n",
    "def score_response(\n",
    "    prompt: str,\n",
    "    response: str,\n",
    "    model: str = JUDGE_MODEL,\n",
    "    retries: int = 3,\n",
    ") -> Optional[dict]:\n",
    "    \"\"\"Score a single response across 4 VJ dimensions.\"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": SCORE_PROMPT.format(prompt=prompt[:600], response=response[:600]),\n",
    "                }],\n",
    "                temperature=0.0,\n",
    "                max_tokens=100,\n",
    "            )\n",
    "            return json.loads(completion.choices[0].message.content.strip())\n",
    "        except Exception as e:\n",
    "            if attempt == retries - 1:\n",
    "                print(f\"Scoring failed: {e}\")\n",
    "                return None\n",
    "            time.sleep(2 ** attempt)\n",
    "\n",
    "\n",
    "# Test\n",
    "test = score_response(\n",
    "    \"My order hasn't arrived after 2 weeks.\",\n",
    "    \"I apologize for the delay. Please contact support@shop.com with your order number.\"\n",
    ")\n",
    "print(\"Test score:\", test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Responses from Each Checkpoint\n",
    "\n",
    "Compare: **base** · **SFT** · **DPO**  \n",
    "Uses the same 200-sample held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "BASE_MODEL = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "RUN_NAME   = \"qwen2.5-7b-customer-support-dpo\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "def load_checkpoint(adapter_path: Optional[str] = None):\n",
    "    \"\"\"Load base model, optionally with LoRA adapter.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL, quantization_config=bnb_config,\n",
    "        device_map=\"auto\", trust_remote_code=True\n",
    "    )\n",
    "    if adapter_path and Path(adapter_path).exists():\n",
    "        model = PeftModel.from_pretrained(model, adapter_path)\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def generate(model, tokenizer, prompt: str, max_new_tokens: int = 200) -> str:\n",
    "    text = f\"### User:\\n{prompt}\\n\\n### Assistant:\\n\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs, max_new_tokens=max_new_tokens,\n",
    "            do_sample=False, temperature=1.0,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    return tokenizer.decode(out[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "print(\"Functions defined. Load checkpoints in next cell when ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test set\n",
    "df_test = pd.read_json(DATA_DIR / \"dpo_weighted.jsonl\", lines=True).sample(200, random_state=99)\n",
    "prompts = df_test[\"prompt\"].tolist()\n",
    "\n",
    "CHECKPOINTS = {\n",
    "    \"base\": None,\n",
    "    \"sft\":  str(MODELS_DIR / f\"{RUN_NAME}-sft-final\"),\n",
    "    \"dpo\":  str(MODELS_DIR / f\"{RUN_NAME}-dpo-final\"),\n",
    "}\n",
    "\n",
    "all_responses = {}\n",
    "\n",
    "for ckpt_name, adapter_path in CHECKPOINTS.items():\n",
    "    print(f\"\\n=== Generating: {ckpt_name} ===\")\n",
    "    model, tokenizer = load_checkpoint(adapter_path)\n",
    "    responses = [generate(model, tokenizer, p) for p in tqdm(prompts)]\n",
    "    all_responses[ckpt_name] = responses\n",
    "    # Free VRAM between checkpoints\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Generation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Score All Responses with VJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIMS = [\"correctness\", \"groundedness\", \"problem_solution\", \"style\"]\n",
    "results = []\n",
    "\n",
    "for ckpt_name, responses in all_responses.items():\n",
    "    print(f\"Scoring: {ckpt_name}...\")\n",
    "    for prompt, response in tqdm(zip(prompts, responses), total=len(prompts)):\n",
    "        scores = score_response(prompt, response)\n",
    "        if scores:\n",
    "            results.append({\"checkpoint\": ckpt_name, \"prompt\": prompt, **scores})\n",
    "\n",
    "df_scores = pd.DataFrame(results)\n",
    "df_scores.to_json(EVAL_DIR / \"vj_scores.jsonl\", orient=\"records\", lines=True)\n",
    "print(f\"Scored {len(df_scores)} rows.\")\n",
    "df_scores.groupby(\"checkpoint\")[DIMS].mean().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Alignment Tax Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_scores = df_scores.groupby(\"checkpoint\")[DIMS].mean()\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "for ax, dim in zip(axes, DIMS):\n",
    "    vals = mean_scores[dim]\n",
    "    colors = [\"#4C72B0\" if v == vals.max() else \"#DD8452\" for v in vals]\n",
    "    ax.bar(vals.index, vals.values, color=colors)\n",
    "    ax.set_title(dim, fontsize=13)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axhline(vals[\"base\"], linestyle=\"--\", color=\"gray\", linewidth=1, label=\"base\")\n",
    "    for i, v in enumerate(vals.values):\n",
    "        ax.text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\", fontsize=10)\n",
    "\n",
    "plt.suptitle(\"VJ Scores: Base → SFT → DPO\", fontsize=15, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(EVAL_DIR / \"alignment_tax.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Flag alignment tax\n",
    "for dim in [\"correctness\", \"problem_solution\"]:\n",
    "    delta = mean_scores.loc[\"dpo\", dim] - mean_scores.loc[\"base\", dim]\n",
    "    if delta < -0.02:\n",
    "        print(f\"⚠  Alignment tax on '{dim}': {delta:+.3f}\")\n",
    "    else:\n",
    "        print(f\"✓  No significant tax on '{dim}': {delta:+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Meta-Eval — Judge-on-Judge Agreement\n",
    "\n",
    "Estimates VJ precision by comparing GPT-4o-mini scores to GPT-4o scores on 500 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "META_EVAL_N = 100   # raise to 500 for production; 100 here to control cost\n",
    "\n",
    "df_meta = df_scores[df_scores[\"checkpoint\"] == \"dpo\"].sample(META_EVAL_N, random_state=7)\n",
    "strong_judge_scores = []\n",
    "\n",
    "print(f\"Running meta-eval on {META_EVAL_N} samples with gpt-4o...\")\n",
    "for _, row in tqdm(df_meta.iterrows(), total=META_EVAL_N):\n",
    "    # Find corresponding response\n",
    "    idx = df_scores[\n",
    "        (df_scores[\"checkpoint\"] == \"dpo\") & (df_scores[\"prompt\"] == row[\"prompt\"])\n",
    "    ].index[0]\n",
    "    response = all_responses[\"dpo\"][prompts.index(row[\"prompt\"])] if row[\"prompt\"] in prompts else \"\"\n",
    "    s = score_response(row[\"prompt\"], response, model=\"gpt-4o\")\n",
    "    if s:\n",
    "        strong_judge_scores.append({\"prompt\": row[\"prompt\"], **{f\"strong_{k}\": v for k, v in s.items()}})\n",
    "\n",
    "df_meta_eval = df_meta.merge(\n",
    "    pd.DataFrame(strong_judge_scores), on=\"prompt\", how=\"inner\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== Judge Agreement (Pearson r): VJ-mini vs GPT-4o ===\")\n",
    "for dim in DIMS:\n",
    "    r, p = stats.pearsonr(df_meta_eval[dim], df_meta_eval[f\"strong_{dim}\"])\n",
    "    quality = \"good\" if r > 0.7 else \"moderate\" if r > 0.5 else \"⚠ LOW\"\n",
    "    print(f\"  {dim:<22} r={r:.3f}  p={p:.3f}  [{quality}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Score Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "for ax, ckpt in zip(axes, [\"base\", \"sft\", \"dpo\"]):\n",
    "    sub = df_scores[df_scores[\"checkpoint\"] == ckpt][DIMS]\n",
    "    sns.heatmap(\n",
    "        sub.corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\",\n",
    "        vmin=-1, vmax=1, ax=ax, cbar=False,\n",
    "    )\n",
    "    ax.set_title(f\"{ckpt} — score correlations\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(EVAL_DIR / \"score_correlations.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Negative style↔correctness correlation in DPO? That's the alignment tax in numbers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Checkpoint | correctness | problem_solution | style | alignment tax? |\n",
    "|------------|-------------|------------------|-------|----------------|\n",
    "| base       | —           | —                | —     | —              |\n",
    "| sft        | ↑           | ↑                | ~     | None           |\n",
    "| dpo        | ±           | ±                | ↑     | Check output   |\n",
    "\n",
    "**Next:** `04_alignment_pain_points.ipynb` — deeper analysis + remedies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
